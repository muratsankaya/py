{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:27:58.541655Z","iopub.execute_input":"2024-12-07T20:27:58.542000Z","iopub.status.idle":"2024-12-07T20:27:58.903581Z","shell.execute_reply.started":"2024-12-07T20:27:58.541969Z","shell.execute_reply":"2024-12-07T20:27:58.902598Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install datasets\n!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:28:00.567150Z","iopub.execute_input":"2024-12-07T20:28:00.567600Z","iopub.status.idle":"2024-12-07T20:28:19.685000Z","shell.execute_reply.started":"2024-12-07T20:28:00.567542Z","shell.execute_reply":"2024-12-07T20:28:19.684110Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.0.0 sacrebleu-2.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:19:58.341100Z","iopub.execute_input":"2024-11-24T19:19:58.341593Z","iopub.status.idle":"2024-11-24T19:19:59.570146Z","shell.execute_reply.started":"2024-11-24T19:19:58.341536Z","shell.execute_reply":"2024-11-24T19:19:59.568854Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wmt14\", \"de-en\")  # WMT-14 English-German\ntrain_data = dataset[\"train\"]\nvalid_data = dataset[\"validation\"]\ntest_data = dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:20:02.627727Z","iopub.execute_input":"2024-11-24T19:20:02.628344Z","iopub.status.idle":"2024-11-24T19:20:26.762498Z","shell.execute_reply.started":"2024-11-24T19:20:02.628305Z","shell.execute_reply":"2024-11-24T19:20:26.761229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d8d64f3814440cba5474406e1678cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa2ae5e79b394c55ac5ea9cfd047ef83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f47d6f00ca4e1d841002feebbdd8c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85fab23d3274d86ad7e9cf85dd624eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a23986f492434a9173a54c17afcd99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73f582a8d22f4b0b9632b0c50a61c6a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b113e17de6b5492e92b8dc779f6be883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7bb13eef3a4db79c5c6f343ebcf5f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6745a36e434b60941ba1f181ff3be6"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:46:33.082161Z","iopub.execute_input":"2024-11-24T19:46:33.082552Z","iopub.status.idle":"2024-11-24T19:46:33.089756Z","shell.execute_reply.started":"2024-11-24T19:46:33.082517Z","shell.execute_reply":"2024-11-24T19:46:33.088459Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['translation'],\n    num_rows: 10\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\n# Initialize a BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n\n# Extract English and German sentences\ncorpus = [example[\"translation\"][\"en\"] for example in train_data] + \\\n         [example[\"translation\"][\"de\"] for example in train_data]\n\n# Train the tokenizer on the corpus\ntrainer = trainers.BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"], vocab_size=32000)\ntokenizer.train_from_iterator(corpus, trainer=trainer)\n\n# Save the tokenizer for future use\ntokenizer.save(\"bpe_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:56:57.876080Z","iopub.execute_input":"2024-11-24T21:56:57.876876Z","iopub.status.idle":"2024-11-24T21:56:58.052769Z","shell.execute_reply.started":"2024-11-24T21:56:57.876817Z","shell.execute_reply":"2024-11-24T21:56:58.051216Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def tokenize_function(example):\n    # Tokenize English and German text\n    src_tokens = tokenizer.encode(example[\"translation\"][\"en\"]).ids\n    tgt_tokens = tokenizer.encode(example[\"translation\"][\"de\"]).ids\n    return {\"src_tokens\": src_tokens, \"tgt_tokens\": tgt_tokens}\n\n# Apply tokenization\ntrain_data = train_data.map(tokenize_function, remove_columns=[\"translation\"])\n# valid_data = valid_data.map(tokenize_function, remove_columns=[\"translation\"])\n# test_data = test_data.map(tokenize_function, remove_columns=[\"translation\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:57:16.140948Z","iopub.execute_input":"2024-11-24T21:57:16.141334Z","iopub.status.idle":"2024-11-24T21:57:16.205829Z","shell.execute_reply.started":"2024-11-24T21:57:16.141301Z","shell.execute_reply":"2024-11-24T21:57:16.204384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"498cabee086946a4a8e40fcb1bfe5897"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:57:28.023769Z","iopub.execute_input":"2024-11-24T21:57:28.024215Z","iopub.status.idle":"2024-11-24T21:57:28.032273Z","shell.execute_reply.started":"2024-11-24T21:57:28.024178Z","shell.execute_reply":"2024-11-24T21:57:28.030978Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['src', 'tgt', 'src_tokens', 'tgt_tokens'],\n    num_rows: 10\n})"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\n# Define a data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n\n# Create PyTorch DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=data_collator)\nvalid_loader = DataLoader(valid_data, batch_size=32, collate_fn=data_collator)\ntest_loader = DataLoader(test_data, batch_size=32, collate_fn=data_collator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:58:18.159516Z","iopub.execute_input":"2024-11-24T21:58:18.159986Z","iopub.status.idle":"2024-11-24T21:58:43.320992Z","shell.execute_reply.started":"2024-11-24T21:58:18.159941Z","shell.execute_reply":"2024-11-24T21:58:43.319602Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a6a19069504201a4014e3ca10fda6e"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"train_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:59:04.621112Z","iopub.execute_input":"2024-11-24T21:59:04.621537Z","iopub.status.idle":"2024-11-24T21:59:04.629127Z","shell.execute_reply.started":"2024-11-24T21:59:04.621495Z","shell.execute_reply":"2024-11-24T21:59:04.627938Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x790836d3fb20>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import torch.nn as nn\n\n# Define the embedding layer\nvocab_size = tokenizer.get_vocab_size()\nd_model = 512  # Dimension of embeddings\nembedding = nn.Embedding(vocab_size, d_model)\n\n# Example usage\nbatch = next(iter(train_loader))  # A batch from the dataloader\nsrc_tokens = batch[\"src_tokens\"]  # Source token IDs\ntgt_tokens = batch[\"tgt_tokens\"]  # Target token IDs\n\nsrc_embeddings = embedding(src_tokens)  # Shape: (batch_size, seq_length, d_model)\ntgt_embeddings = embedding(tgt_tokens)  # Shape: (batch_size, seq_length, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:59:26.955099Z","iopub.execute_input":"2024-11-24T21:59:26.955635Z","iopub.status.idle":"2024-11-24T21:59:27.656033Z","shell.execute_reply.started":"2024-11-24T21:59:26.955548Z","shell.execute_reply":"2024-11-24T21:59:27.654315Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# A batch from the dataloader\u001b[39;00m\n\u001b[1;32m     10\u001b[0m src_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Source token IDs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tgt_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Target token IDs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'pad'"],"ename":"AttributeError","evalue":"'tokenizers.Tokenizer' object has no attribute 'pad'","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport torch\nimport numpy as np\n\n\ndef softmax(x: torch.Tensor, d: int):\n    # return torch.nn.Softmax(dim=d)(x)\n    return torch.nn.functional.softmax(x, dim=d)\n\n\nclass FeedForward(torch.nn.Module):\n    def __init__(\n        self,\n        d_model: int = 512,\n        hidden_size: list[tuple[int]] = None,\n        activation: torch.nn.Module = torch.nn.ReLU(),\n    ):\n        \"\"\"\n        notes:\n            - default dimensions are set according to paper\n\n        \"\"\"\n        if hidden_size is None:\n            hidden_size = [(d_model, 2048), (2048, d_model)]\n\n        assert len(hidden_size) > 0, \"hidden_size must be greater than 0\"\n        assert (\n            hidden_size[0][0] == d_model and hidden_size[-1][1] == d_model\n        ), \"input and output dimensions must equal d_model\"\n\n        super().__init__()\n\n        layers = []\n        n = len(hidden_size)\n        for i in range(n):\n            layers.append(torch.nn.Linear(*hidden_size[i]))\n\n            # No activation after the final layer\n            if i < n - 1:\n                layers.append(activation)\n\n        self.net = torch.nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n\ndef attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    pad_mask: torch.Tensor,\n    mask: torch.Tensor = None,\n):\n    \"\"\"\n    input:\n        q: a torch tensor of size: (batch_size*h, seq_len, d_k)\n        k: a torch tensor of size: (batch_size*h, seq_len, d_k)\n        v: a torch tensor of size: (batch_size*h, seq_len, d_v)\n        pad_mask: a torch tensor of size: (batch_size*h, seq_len, seq_len)\n        mask: a torch tensor of size: (seq_len, seq_len)\n\n    output:\n        A torch tensor of size: (batch_size*h, seq_len, d_v)\n\n    \"\"\"\n    x = torch.matmul(q, torch.transpose(k, 1, 2))\n\n    # Scale x by sqrt(d_k)\n    x = x / math.sqrt(q.size(2))\n\n    if mask is not None:\n\n        # Broadcasting will match the size at dim=0\n        mask = mask.unsqueeze(0)\n\n        # Apply mask via element-wise addition\n        x = x + mask\n\n    x = x + pad_mask\n\n    return torch.matmul(softmax(x, d=2), v)\n\n\nclass MultiHead(torch.nn.Module):\n    def __init__(self, h: int = 8, d_model: int = 512, mask: torch.Tensor = None):\n        \"\"\"\n        input:\n            h: number of heads\n            d_model: model dimensions, i.e. embedding size\n            mask: a boolean to apply masked multi-head attention\n        notes:\n            - default dimensions are set according to the paper\n\n        \"\"\"\n        assert d_model % h == 0, \"d_model must be divisible by h\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // h\n        self.d_v = self.d_k\n        self.h = h\n        self.mask = mask\n\n        self.w_q = torch.nn.Linear(d_model, d_model)\n        self.w_k = torch.nn.Linear(d_model, d_model)\n        self.w_v = torch.nn.Linear(d_model, d_model)\n        self.w_o = torch.nn.Linear(d_model, d_model)\n\n    def forward(\n        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, pad_mask: torch.Tensor\n    ):\n        \"\"\"\n        input:\n            q: a torch tensor of size: (batch_size, seq_len, d_model)\n            k: a torch tensor of size: (batch_size, seq_len, d_model)\n            v: a torch tensor of size: (batch_size, seq_len, d_model)\n            pad_mask: a torch tensor of size: (batch_size, seq_len, seq_len)\n\n        output:\n            A torch tensor of size: (batch_size, seq_len, d_model)\n\n        notes:\n            - d_model is essentially embedding dimensions\n        \"\"\"\n        batch_size, seq_len, _ = q.size()\n        q_h = self.w_q(q)\n        k_h = self.w_k(k)\n        v_h = self.w_v(v)\n\n        # Splitting q, k and v tensors in to h heads\n        q_h = q_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        k_h = k_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        v_h = v_h.reshape(batch_size, seq_len, self.h, self.d_v).transpose(1, 2)\n\n        # Combine heads for parrallel computation\n        q_h = q_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        k_h = k_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        v_h = v_h.reshape(batch_size * self.h, seq_len, self.d_v)\n\n        # Apply attention\n        scores = attention(q_h, k_h, v_h, pad_mask, mask=self.mask)\n\n        # Seperate heads\n        scores = scores.reshape(batch_size, self.h, seq_len, self.d_v).transpose(1, 2)\n\n        # Concat h heads (Concat(head1, ..., headh))\n        scores = scores.reshape(batch_size, seq_len, self.h * self.d_v)\n\n        return self.w_o(scores)\n\n\n# TODO: can change the definition so that if dropout called with\n# p=None then can just terminate. This would make the Encoder and\n# Decoder forward passes more concise\ndef dropout(x: torch.Tensor, p: float = 0.1):\n    assert 0 <= p <= 1, \"p must be a probability\"\n\n    if p == 1:\n        # All elements are dropped; just return zeros.\n        return torch.zeros_like(x)\n\n    # Create a tensor with the same shape as x\n    # and set all is values to 1 - p\n    mask = torch.full_like(x, 1 - p)\n\n    # Will sample the entries from the bernoulli distribution.\n    # The i'th entry of the output tensor will draw a value 1 according\n    # to the i'th probability given the input tensor.\n    mask = torch.bernoulli(mask).to(x.device)\n\n    # Apply dropout via element-wise multiplication.\n    x = x * mask\n\n    # Apply inverted scaling\n    return x * (1 / (1 - p))\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, number_of_heads: int = 8, d_model: int = 512):\n        \"\"\"\n        notes:\n            - default parameter values are based on the paper\n        \"\"\"\n        assert (\n            d_model % number_of_heads == 0\n        ), \"d_model must be divisible by number_of_heads\"\n        super().__init__()\n        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n        self.feed_forward = FeedForward(d_model=d_model)\n        self.d_model = d_model\n        self.layer_norms = torch.nn.ModuleList(\n            torch.nn.LayerNorm(d_model) for _ in range(2)\n        )\n\n    def forward(\n        self, x: torch.Tensor, pad_mask: torch.Tensor, dropout_p=None\n    ) -> torch.Tensor:\n        \"\"\"\n        input:\n            x: a torch tensor of size: (batch_size, seq_len, d_model)\n        output:\n            a torch tensor of size: (batch_size, seq_len, d_model)\n        \"\"\"\n\n        x = self.layer_norms[0](\n            x\n            + (\n                self.multi_head_attention(x, x, x, pad_mask)\n                if dropout_p is None\n                else dropout(self.multi_head_attention(x, x, x, pad_mask), p=dropout_p)\n            )\n        )\n\n        return self.layer_norms[1](\n            x\n            + (\n                self.feed_forward(x)\n                if dropout_p is None\n                else dropout(self.feed_forward(x), p=dropout_p)\n            )\n        )\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(\n        self,\n        attention_mask: torch.Tensor,\n        number_of_heads: int = 8,\n        d_model: int = 512,\n    ):\n        \"\"\"\n        notes:\n            - default parameter values are based on the paper\n            - in the previous implementation we were using the same multi head\n            module and inidicating the apply mask on the forward pass. That's an\n            outregous mistake. The same parameters are being used in that case\n            only when applying attention part of the sequence was getting masked.\n        \"\"\"\n        assert (\n            d_model % number_of_heads == 0\n        ), \"d_model must be divisible by number_of_heads\"\n        super().__init__()\n        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n        self.masked_multi_head_attention = MultiHead(\n            h=number_of_heads, d_model=d_model, mask=attention_mask\n        )\n        self.feed_forward = FeedForward(d_model=d_model)\n        self.d_model = d_model\n        self.layer_norms = torch.nn.ModuleList(\n            torch.nn.LayerNorm(d_model) for _ in range(3)\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        encoder_x: torch.Tensor,\n        decoder_pad_mask: torch.Tensor,\n        encoder_pad_mask: torch.Tensor,\n        dropout_p=None,\n    ) -> torch.Tensor:\n        \"\"\"\n        inputs:\n            x: a torch tensor of size: (batch_size, seq_len, d_model)\n            encoder_x: a torch tensor of size: (batch_size, seq_len, d_model)\n        output:\n            a torch tensor of size: (batch_size, seq_len, d_model)\n        \"\"\"\n        x = self.layer_norms[0](\n            x\n            + (\n                self.masked_multi_head_attention(x, x, x, decoder_pad_mask)\n                if dropout_p is None\n                else dropout(\n                    self.masked_multi_head_attention(x, x, x, decoder_pad_mask),\n                    p=dropout_p,\n                )\n            )\n        )\n\n        # Cross-Attention\n        # Here encoder_pad_mask must be used because\n        x = self.layer_norms[1](\n            x\n            + (\n                self.multi_head_attention(x, encoder_x, encoder_x, encoder_pad_mask)\n                if dropout_p is None\n                else dropout(\n                    self.multi_head_attention(\n                        x, encoder_x, encoder_x, encoder_pad_mask\n                    ),\n                    p=dropout_p,\n                )\n            )\n        )\n\n        return self.layer_norms[2](\n            x\n            + (\n                self.feed_forward(x)\n                if dropout_p is None\n                else dropout(self.feed_forward(x), p=dropout_p)\n            )\n        )\n\n\ndef positional_encodings(seq_len: int, d_model: int) -> np.ndarray:\n    pos, i = np.indices((seq_len, d_model))\n    return np.where(\n        i % 2 == 0,\n        np.sin(pos / np.power(10000, (2 * i / d_model))),\n        np.cos(pos / np.power(10000, (2 * i / d_model))),\n    )\n\n\ndef get_attention_mask(seq_len: int, dtype: torch.dtype) -> torch.Tensor:\n    \"\"\"\n    Here we will be creating a triangular matrix where\n    all the upper triangle (above the diagonal) is set to -oo.\n\n    output:\n        A torch tensor of size: (seq_len, seq_len)\n    \"\"\"\n\n    mask_x = torch.full((seq_len, seq_len), float(\"-inf\"), dtype=dtype)\n\n    return torch.triu(mask_x, diagonal=1)\n\n    # Add a new batch_size dimension and expand it to\n    # match batch_size\n    # -1 means keep the size at that dimension\n    # However its also documented here:\n    # https://stackoverflow.com/questions/65900110/does-pytorch-broadcast-consume-less-memory-than-expand\n    # that expand does not also consume extra memory\n    # mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n\n\ndef get_pad_mask(\n    x: torch.Tensor, h: int, batch_size: int, seq_len: int, pad_token_id: int = 0\n) -> torch.Tensor:\n    \"\"\"\n    inputs:\n        x: a torch tensor of size: (batch_size, seq_len)\n    \"\"\"\n    # 0 must be the padding token id\n    # Here torch.where(encoder_x == 0, float(\"-inf\"), 0)\n    #    .unsqueeze(2)\n    #    .expand(-1, -1, self.seq-len)\n    # should also mathematically produce the same shape\n    # but typically the keys gets masked so the current approach\n    # aligns better with the paper\n    assert x.size() == (\n        batch_size,\n        seq_len,\n    ), f\"x must have size: {(batch_size, seq_len)}\"\n\n    return (\n        torch.where(x == pad_token_id, float(\"-inf\"), 0)\n        .unsqueeze(1)\n        .unsqueeze(1)\n        .expand(-1, h, seq_len, -1)\n        .reshape(batch_size * h, seq_len, seq_len)\n    )\n\n\nclass Transformer(torch.nn.Module):\n    def __init__(\n        self,\n        batch_size: int,\n        seq_len: int,\n        d_model: int = 512,\n        n: int = 6,\n        number_of_heads: int = 8,\n        input_vocab_size: int = 37000,\n        output_vocab_size: int = 37000,\n        dtype: torch.dtype = torch.float32,\n        dropout_p: float = None,\n    ):\n        \"\"\"\n        inputs:\n            n: the number of encoder and decoder stacks\n            d_model: model dimensions, i.e. embedding dimensions\n            number_of_heads: number of heads using in multi-head attention\n            input_vocab_size: size of the vocabulary for the input structure\n            output_vocab_size: size of the vocabulary for the output structure\n\n        notes:\n            - I'm using a fixed seq_len for both the input and the output. That could\n            be adjusted to make it varied and more flexible.\n\n        \"\"\"\n        super().__init__()\n        self.batch_size = batch_size\n        self.seq_len = seq_len\n        self.n = n\n        self.d_model = d_model\n        self.number_of_heads = number_of_heads\n        self.vocab_size = output_vocab_size\n        self.dtype = dtype\n        self.register_buffer(\n            \"pe\",\n            torch.from_numpy(positional_encodings(seq_len, d_model))\n            .to(dtype)\n            .unsqueeze(0),\n        )\n        self.register_buffer(\"attention_mask\", get_attention_mask(seq_len, dtype=dtype))\n        self.linear = torch.nn.Linear(d_model, output_vocab_size)\n        assert (\n            dropout_p is None or 0 <= dropout_p <= 1\n        ), \"p_dropout must be a value between 0 and 1\"\n        self.dropout_p = dropout_p\n\n        self.input_embedding = torch.nn.Embedding(\n            num_embeddings=input_vocab_size, embedding_dim=d_model\n        )\n        self.output_embedding = torch.nn.Embedding(\n            num_embeddings=output_vocab_size, embedding_dim=d_model\n        )\n\n        # Using ModuleList is crucial here instead of python list\n        # If python list is used, then model.parameters() will not\n        # return the paremeters of layers inside.\n        self.encoders = torch.nn.ModuleList(\n            Encoder(number_of_heads=number_of_heads, d_model=d_model)\n            for _ in range(self.n)\n        )\n\n        self.decoders = torch.nn.ModuleList(\n            Decoder(\n                attention_mask=self.attention_mask,\n                number_of_heads=number_of_heads,\n                d_model=d_model,\n            )\n            for _ in range(self.n)\n        )\n\n    def forward(\n        self,\n        encoder_x: torch.Tensor,\n        decoder_x: torch.Tensor,\n        apply_softmax: bool = False,\n        pad_token_id: int = 0,\n    ):\n        \"\"\"\n        inputs:\n            encoder_x: a torch tensor of size: (batch_size, seq_len)\n            decoder_x: a torch tensor of size: (batch_size, seq_len)\n            apply_softmax: a boolean.\n                - Most torch loss functions expect logits instead of porbabilities.\n                So make sure that the loss function does not normalize inputs and\n                expects probabilities before setting this to True.\n\n        outputs:\n            a probability distribution over the vocabulary if apply_softmax is true else\n            it outputs logits\n        \"\"\"\n        assert (\n            encoder_x.size() == decoder_x.size() == (self.batch_size, self.seq_len)\n        ), f\"encoder_x and decoder_x must both have the size: ({self.batch_size}, {self.seq_len})\"\n\n        assert torch.any(encoder_x) and torch.any(decoder_x), (\n            \"empty examples are not allowed. There could be some additional reasons \"\n            \"for not to allow them but simple example why is that softmax is not defined \"\n            \"over an empty sequence\"\n        )\n\n        encoder_x_pad_mask = get_pad_mask(\n            encoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n            pad_token_id=pad_token_id,\n        )\n        decoder_x_pad_mask = get_pad_mask(\n            decoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n            pad_token_id=pad_token_id,\n        )\n\n        # In the paper it is mentioned that they scale the embedding weights by math.sqrt(self.d_model)\n        # see the end of section 3.4 for more detail\n        encoder_x = self.input_embedding(encoder_x) * math.sqrt(self.d_model) + self.pe\n        decoder_x = self.output_embedding(decoder_x) * math.sqrt(self.d_model) + self.pe\n\n        for encoder in self.encoders:\n            encoder_x = encoder(encoder_x, encoder_x_pad_mask, dropout_p=self.dropout_p)\n\n        for decoder in self.decoders:\n            decoder_x = decoder(\n                decoder_x,\n                encoder_x,\n                decoder_pad_mask=decoder_x_pad_mask,\n                encoder_pad_mask=encoder_x_pad_mask,\n                dropout_p=self.dropout_p,\n            )\n\n        return (\n            softmax(self.linear(decoder_x), d=2)\n            if apply_softmax\n            else self.linear(decoder_x)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:30:20.193079Z","iopub.execute_input":"2024-12-11T01:30:20.193523Z","iopub.status.idle":"2024-12-11T01:30:20.234068Z","shell.execute_reply.started":"2024-12-11T01:30:20.193493Z","shell.execute_reply":"2024-12-11T01:30:20.233346Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Use the CountingDataset\nclass CountingDataset(torch.utils.data.Dataset):\n    def __init__(self, n, max_length=8, vocab_size=3):\n        assert vocab_size > 2\n        self.n = n\n        self.vocab_size = vocab_size\n        seq_lengths = np.random.randint(max_length // 2, max_length, n)\n        data = np.random.randint(1, vocab_size, (n, max_length))\n\n        # Replace elements past the sequence length with 0 (padding token)\n        for i in range(n):\n            data[i, seq_lengths[i]:] = 0\n\n        # Labels: whether `1`s outnumber `2`s\n        num_ones = (data == 1).sum(axis=1)\n        num_twos = (data == 2).sum(axis=1)\n        self.data = torch.tensor(data, dtype=torch.long)\n        self.labels = torch.tensor((num_ones > num_twos).astype(int), dtype=torch.long)\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Create dataset and dataloader\ndataset = CountingDataset(1000, max_length=8, vocab_size=3)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n\n# Transformer Model Parameters\nbatch_size = 32\nseq_len = 8\nd_model = 16\nnumber_of_heads = 2\nn = 2\ninput_vocab_size = 3  # Matches CountingDataset\noutput_vocab_size = 2  # Binary classification (0 or 1)\n\n# Instantiate Transformer Model\nmodel = Transformer(\n    batch_size=batch_size,\n    seq_len=seq_len,\n    d_model=d_model,\n    n=n,\n    number_of_heads=number_of_heads,\n    input_vocab_size=input_vocab_size,\n    output_vocab_size=output_vocab_size,\n)\n\n# Define Loss and Optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop\nepochs = 120\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for x, y in dataloader:\n        # Initialize decoder input with start-of-sequence tokens\n        decoder_x = torch.zeros_like(x)\n        decoder_x[:, 0] = 1  # Start-of-sequence token\n\n        # Forward pass\n        output = model(x, decoder_x)\n\n        # Use output at the last position for classification\n        logits = output[:, -1, :]  # Shape: (batch_size, output_vocab_size)\n        loss = criterion(logits, y)\n\n        total_loss += loss.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n\n# Testing the Model\nmodel.eval()\nwith torch.no_grad():\n    for test_data, test_labels in dataloader:\n        if test_data.size(0) != batch_size:\n            continue\n\n        decoder_x = torch.zeros_like(test_data)\n        decoder_x[:, 0] = 1  # Start-of-sequence token\n        output = model(test_data, decoder_x, apply_softmax=True)\n        predictions = torch.argmax(output[:, -1, :], dim=-1)\n\n        print(\"Test Labels:\", test_labels.numpy())\n        print(\"Predictions:\", predictions.numpy())\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:53:10.943726Z","iopub.execute_input":"2024-12-10T20:53:10.944452Z","iopub.status.idle":"2024-12-10T20:54:50.430440Z","shell.execute_reply.started":"2024-12-10T20:53:10.944418Z","shell.execute_reply":"2024-12-10T20:54:50.429506Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/120, Loss: 0.28974135328204403\nEpoch 2/120, Loss: 0.08789960349038724\nEpoch 3/120, Loss: 0.026533257636812427\nEpoch 4/120, Loss: 0.016345204274740913\nEpoch 5/120, Loss: 0.011934845978694578\nEpoch 6/120, Loss: 0.009116222931733054\nEpoch 7/120, Loss: 0.007177581755264151\nEpoch 8/120, Loss: 0.005799303402102763\nEpoch 9/120, Loss: 0.004775385687788648\nEpoch 10/120, Loss: 0.004000444964115177\nEpoch 11/120, Loss: 0.0033937536946107303\nEpoch 12/120, Loss: 0.0029148882435214136\nEpoch 13/120, Loss: 0.002529515937391308\nEpoch 14/120, Loss: 0.0022183142362102387\nEpoch 15/120, Loss: 0.0019572653597401033\nEpoch 16/120, Loss: 0.0017383652739226818\nEpoch 17/120, Loss: 0.001551750345125554\nEpoch 18/120, Loss: 0.0013946275407027814\nEpoch 19/120, Loss: 0.0012597408213262115\nEpoch 20/120, Loss: 0.0011412799884114534\nEpoch 21/120, Loss: 0.001037704240110132\nEpoch 22/120, Loss: 0.0009479788150788556\nEpoch 23/120, Loss: 0.0008678083371881756\nEpoch 24/120, Loss: 0.0007980144319815501\nEpoch 25/120, Loss: 0.0007355176963873448\nEpoch 26/120, Loss: 0.0006793117650123613\nEpoch 27/120, Loss: 0.0006290440440117832\nEpoch 28/120, Loss: 0.0005830216518182668\nEpoch 29/120, Loss: 0.0005419288908371762\nEpoch 30/120, Loss: 0.0005046088791136901\nEpoch 31/120, Loss: 0.0004704586819805686\nEpoch 32/120, Loss: 0.00043989244318987813\nEpoch 33/120, Loss: 0.00041145910677920666\nEpoch 34/120, Loss: 0.0003855090011887613\nEpoch 35/120, Loss: 0.00036112983681021196\nEpoch 36/120, Loss: 0.0003394636047643519\nEpoch 37/120, Loss: 0.00031874390345277084\nEpoch 38/120, Loss: 0.000300381256590387\nEpoch 39/120, Loss: 0.00028297043454292563\nEpoch 40/120, Loss: 0.0002668352972250432\nEpoch 41/120, Loss: 0.00025193046284614193\nEpoch 42/120, Loss: 0.00023793282349699087\nEpoch 43/120, Loss: 0.00022502202726525043\nEpoch 44/120, Loss: 0.0002128307340523949\nEpoch 45/120, Loss: 0.0002011412425124417\nEpoch 46/120, Loss: 0.0001905966214948304\nEpoch 47/120, Loss: 0.00018064673584435256\nEpoch 48/120, Loss: 0.00017127224888216944\nEpoch 49/120, Loss: 0.00016258434052654212\nEpoch 50/120, Loss: 0.0001544226148736573\nEpoch 51/120, Loss: 0.00014679717940218267\nEpoch 52/120, Loss: 0.00013944839837119703\nEpoch 53/120, Loss: 0.00013257657326081948\nEpoch 54/120, Loss: 0.0001260510992236255\nEpoch 55/120, Loss: 0.00012000512933516274\nEpoch 56/120, Loss: 0.00011414331768924791\nEpoch 57/120, Loss: 0.00010875765198556286\nEpoch 58/120, Loss: 0.00010360915081447831\nEpoch 59/120, Loss: 9.874936011094119e-05\nEpoch 60/120, Loss: 9.418033005204052e-05\nEpoch 61/120, Loss: 8.984859150293613e-05\nEpoch 62/120, Loss: 8.566523460866583e-05\nEpoch 63/120, Loss: 8.169574392659049e-05\nEpoch 64/120, Loss: 7.806640942656105e-05\nEpoch 65/120, Loss: 7.455145978676756e-05\nEpoch 66/120, Loss: 7.116206125506471e-05\nEpoch 67/120, Loss: 6.805167105349321e-05\nEpoch 68/120, Loss: 6.50301874209676e-05\nEpoch 69/120, Loss: 6.223244042391138e-05\nEpoch 70/120, Loss: 5.949885176490962e-05\nEpoch 71/120, Loss: 5.687231966857648e-05\nEpoch 72/120, Loss: 5.445246455154472e-05\nEpoch 73/120, Loss: 5.2149400993761035e-05\nEpoch 74/120, Loss: 4.991098338054613e-05\nEpoch 75/120, Loss: 4.7883325967104026e-05\nEpoch 76/120, Loss: 4.5812286243158124e-05\nEpoch 77/120, Loss: 4.3883636422951014e-05\nEpoch 78/120, Loss: 4.2067699870395086e-05\nEpoch 79/120, Loss: 4.034860971695443e-05\nEpoch 80/120, Loss: 3.8696574554177784e-05\nEpoch 81/120, Loss: 3.7140066260396834e-05\nEpoch 82/120, Loss: 3.565072420901889e-05\nEpoch 83/120, Loss: 3.417219962894676e-05\nEpoch 84/120, Loss: 3.281239796412973e-05\nEpoch 85/120, Loss: 3.151988312311005e-05\nEpoch 86/120, Loss: 3.025212408242477e-05\nEpoch 87/120, Loss: 2.909371564343136e-05\nEpoch 88/120, Loss: 2.7988417763657296e-05\nEpoch 89/120, Loss: 2.6874708524818772e-05\nEpoch 90/120, Loss: 2.5816754655792346e-05\nEpoch 91/120, Loss: 2.4831141995344942e-05\nEpoch 92/120, Loss: 2.3926278381916363e-05\nEpoch 93/120, Loss: 2.301180399128146e-05\nEpoch 94/120, Loss: 2.2111985609010462e-05\nEpoch 95/120, Loss: 2.1315393936557455e-05\nEpoch 96/120, Loss: 2.053141683826388e-05\nEpoch 97/120, Loss: 1.9773637529485108e-05\nEpoch 98/120, Loss: 1.9034362799908606e-05\nEpoch 99/120, Loss: 1.8349522501774764e-05\nEpoch 100/120, Loss: 1.7707102092719005e-05\nEpoch 101/120, Loss: 1.706588286211744e-05\nEpoch 102/120, Loss: 1.6428509649901365e-05\nEpoch 103/120, Loss: 1.584689298103894e-05\nEpoch 104/120, Loss: 1.5285224582194425e-05\nEpoch 105/120, Loss: 1.4737015414040446e-05\nEpoch 106/120, Loss: 1.4247807220888188e-05\nEpoch 107/120, Loss: 1.3739854782010474e-05\nEpoch 108/120, Loss: 1.3280207836472865e-05\nEpoch 109/120, Loss: 1.2820319933516363e-05\nEpoch 110/120, Loss: 1.2394080811411127e-05\nEpoch 111/120, Loss: 1.19737290245417e-05\nEpoch 112/120, Loss: 1.156659653779277e-05\nEpoch 113/120, Loss: 1.1174844046528901e-05\nEpoch 114/120, Loss: 1.0793546871648306e-05\nEpoch 115/120, Loss: 1.044998316501733e-05\nEpoch 116/120, Loss: 1.0105457946455346e-05\nEpoch 117/120, Loss: 9.779078320735284e-06\nEpoch 118/120, Loss: 9.455221193239858e-06\nEpoch 119/120, Loss: 9.13713225717431e-06\nEpoch 120/120, Loss: 8.853531565569761e-06\nTest Labels: [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\nPredictions: [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:31:16.365302Z","iopub.execute_input":"2024-12-07T20:31:16.365700Z","iopub.status.idle":"2024-12-07T20:31:17.262836Z","shell.execute_reply.started":"2024-12-07T20:31:16.365666Z","shell.execute_reply":"2024-12-07T20:31:17.261805Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import time\n\nimport numpy as np\nimport torch\n\n\nimport os\nimport torch\n\n\ndef save(filename, **kwargs):\n    \"\"\"\n    Save a pytorch object to file\n    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n    Arguments:\n        filename: the file in which to save the object\n\n    Possible keyword arguments (kwargs):\n        epoch: the epoch so far if training\n        model_state_dict: a model's state\n        opt_state_dict: a optimizer's state, if training\n    \"\"\"\n\n    msg = f\"{filename} exists: delete it first to replace it.\"\n    assert not os.path.exists(filename), msg\n    torch.save(kwargs, filename)\n\n\ndef load(filename):\n    \"\"\"\n    Load a pytorch object from a given filename\n    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n    You shouldn't need to edit this function.\n\n    Arguments:\n        filename: the file from which to load the object\n    \"\"\"\n\n    return torch.load(filename)\n\n\nclass Trainer:\n    def __init__(self, optimizer, model, loss_func, scheduler=None, **kwargs):\n        \"\"\"\n        Initialize the optimizer for the model, using any necessary kwargs\n        Save the model and loss function for later calculation\n        You shouldn't need to edit this function.\n        \"\"\"\n\n        self.optimizer: torch.optim.Optimizer = optimizer(model.parameters(), **kwargs)\n        self.scheduler = scheduler[\"scheduler\"](\n            self.optimizer, lr_lambda=scheduler[\"lr_lambda\"]\n        )\n        self.model: torch.nn.Module = model\n        self.loss_func = loss_func\n\n        self.epoch = 0\n        self.start_time = None\n\n    def run_one_batch(self, encder_x, decoder_x, y, train=True, pad_token_id=0):\n        \"\"\"\n        Run self.model on one batch of data, using `self.loss_func` to\n            compute the model's loss.\n\n        If train=True (the default), you should use `self.optimizer`\n            to update the parameters of `self.model`.\n\n        You should also call `self.optimizer.zero_grad()`; see\n            https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n            for a guide as to when to do that.\n\n        Args\n            enocder_x: the batch's enocder input\n            decoder_x: the batch's decoder input\n            y: the batch's target\n\n        Returns\n            loss: the model's loss on this batch\n        \"\"\"\n\n        if train:\n            self.optimizer.zero_grad()\n\n        outputs = self.model(encder_x, decoder_x, pad_token_id=pad_token_id)\n        loss = self.loss_func(outputs.transpose(1, 2), y)\n\n        if train:\n            loss.backward()\n            self.optimizer.step()\n            if self.scheduler is not None:\n                self.scheduler.step()\n\n        return loss.detach().numpy()\n\n    def run_one_epoch(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        train=True,\n        verbose=False,\n        pad_token_id=0,\n    ):\n        \"\"\"\n        Train one epoch, a batch at a time, using self.run_one_batch\n        You shouldn't need to edit this function.\n\n        Args:\n            data_loader: a torch.utils.data.DataLoader with our dataset\n            stats: an optional dict of information to print out\n\n        Returns:\n            total_loss: the average loss per example\n        \"\"\"\n        np.random.seed(0)\n        torch.manual_seed(0)\n        # torch.use_deterministic_algorithms(True)\n        if self.start_time is None:\n            self.start_time = time.time()\n\n        epoch_size = 0\n        total_loss = 0\n        for batch_idx, batch_data in enumerate(data_loader):\n            encoder_x, decoder_x, y = (\n                batch_data[\"input_ids\"],\n                batch_data[\"decoder_input_ids\"],\n                batch_data[\"labels\"],\n            )\n            epoch_size += encoder_x.size(0)\n            loss = self.run_one_batch(\n                encoder_x, decoder_x, y, train=train, pad_token_id=pad_token_id\n            )\n            total_loss += loss\n\n        avg_loss = total_loss / epoch_size\n\n        if verbose:\n            epoch = self.epoch + 1\n            duration = (time.time() - self.start_time) / 60\n\n            if train:\n                log = [f\"Epoch: {epoch:6d}\"]\n            else:\n                log = [\"Eval:\" + \" \" * 8]\n\n            log.extend(\n                [\n                    f\"Loss: {avg_loss:6.3f}\",\n                    f\"in {duration:5.1f} min\",\n                ]\n            )\n            print(\"  \".join(log))\n\n        return avg_loss\n\n    def train(\n        self, data_loader, n_epochs, train=True, report_every=None, pad_token_id=0\n    ):\n        \"\"\"\n        Run the model for `n_epochs` epochs on the data in `data_loader`\n        You shouldn't need to edit this function.\n\n        Args\n            data_loader: data loader for our data\n            n_epochs: how many epochs to run\n            train: if True, train the model; otherwise, just evaluate it\n            report_every: how often to print out stats\n\n        Returns\n            losses: average loss per epoch\n        \"\"\"\n        self.start_time = time.time()\n\n        if report_every is None:\n            report_every = max(1, n_epochs // 10)\n\n        losses = []\n        for i in range(n_epochs):\n            verbose = ((i + 1) % report_every) == 0\n            loss = self.run_one_epoch(\n                data_loader, train=train, verbose=verbose, pad_token_id=pad_token_id\n            )\n            losses.append(loss)\n            if train:\n                self.epoch += 1\n\n        return losses\n\n    def eval(self, data_loader):\n        \"\"\"\n        Helper function to run through the data loader once and just\n            compute the loss\n        You shouldn't need to edit this function.\n        \"\"\"\n        return self.train(data_loader, 1, train=False, report_every=1)\n\n    def save_trainer(self, filename):\n        \"\"\"\n        Use `src.utils.save` to save this Trainer to file.\n        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n        Args\n            filename: the file to which to save the trainer\n        \"\"\"\n        save(\n            filename,\n            epoch=self.epoch,\n            model_state_dict=self.model.state_dict(),\n            optimizer_state_dict=self.optimizer.state_dict(),\n        )\n\n    def load_trainer(self, filename):\n        \"\"\"\n        Use `src.utils.load` to load this trainer from file.\n        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n        Note: in addition to simply loading the saved model, you must\n            use the information from that checkpoint to update the model's\n            state.\n\n        Args\n            filename: the file from which to load the model\n        \"\"\"\n        checkpoint = load(filename)\n        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        self.epoch = checkpoint[\"epoch\"]\n\n        # this ensures that the trainer's/model's parameters are in training mode\n        self.model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:45:20.184565Z","iopub.execute_input":"2024-12-10T20:45:20.185351Z","iopub.status.idle":"2024-12-10T20:45:20.204050Z","shell.execute_reply.started":"2024-12-10T20:45:20.185315Z","shell.execute_reply":"2024-12-10T20:45:20.203018Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nLoads the wmt12 german-to-englis (de-en) variable from huggging \nface or cache (if its already downloads before)\nand stores it in a variable called wmt14\n\n\"\"\"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\n\nwmt14 = load_dataset(\"wmt14\", \"de-en\")\n\n# Display available splits\nprint(wmt14)\n\n# Display column names and data types for the 'train' split\nprint(wmt14[\"train\"].features)\n\n# Display the first 5 rows of the 'train' split\nfor i in range(5):\n    print(wmt14[\"train\"][i])\n\n# # Convert the 'train' split to a DataFrame\n# df_train = pd.DataFrame(wmt14[\"train\"])\n\n# # Display the first 5 rows\n# print(df_train.head())\n\n# # Calculate sentence lengths\n# en_lengths = [len(ex[\"translation\"][\"en\"].split()) for ex in wmt14[\"train\"]]\n# de_lengths = [len(ex[\"translation\"][\"de\"].split()) for ex in wmt14[\"train\"]]\n\n# # Plot histograms\n# plt.hist(en_lengths, bins=50, alpha=0.5, label=\"English\")\n# plt.hist(de_lengths, bins=50, alpha=0.5, label=\"German\")\n# plt.xlabel(\"Sentence Length\")\n# plt.ylabel(\"Frequency\")\n# plt.legend(loc=\"upper right\")\n# plt.title(\"Sentence Length Distribution\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:30:29.956485Z","iopub.execute_input":"2024-12-11T01:30:29.956790Z","iopub.status.idle":"2024-12-11T01:30:47.646209Z","shell.execute_reply.started":"2024-12-11T01:30:29.956763Z","shell.execute_reply":"2024-12-11T01:30:47.645391Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4336827a20490ab81dad5767cc3afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fba05f1d98140e5b8513de537db2bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6f2936036340108a6ec62bd54462c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069575308e0440109b3a8cb4612a029e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67608407379a4aa6b650fa2521ed0a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74aa8e6570014ee7963845161259b33f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f7bef953d24aa1898fa87364b2d484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d84cd37bf8b4b36a87bb8a137b442a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f0d58ce5904a179fde8ffb76f1f1d6"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 4508785\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 3003\n    })\n})\n{'translation': Translation(languages=['de', 'en'], id=None)}\n{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n{'translation': {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.', 'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'}}\n{'translation': {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}}\n{'translation': {'de': 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.', 'en': 'You have requested a debate on this subject in the course of the next few days, during this part-session.'}}\n{'translation': {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.', 'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nThis module provides functionality to preprocess and tokenize examples from \nthe WMT14 dataset for training a transformer model.\n\nThe WMT14 dataset is a collection of parallel sentences in German and English. \nThe dataset is structured as follows:\n\nDatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 4508785\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 3003\n    })\n})\n\nEach example in the dataset is a dictionary with a 'translation' key, which \ncontains a dictionary with 'de' (German) and 'en' (English) keys:\n\n{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n{'translation': {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode \ndes Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel \nund hoffe, daß Sie schöne Ferien hatten.', 'en': 'I declare resumed the session of the European Parliament \nadjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope \nthat you enjoyed a pleasant festive period.'}}\n\nThis module converts these examples into tokenized inputs and decoder inputs suitable for \ntraining a transformer model. The preprocessing steps include:\n\n1. Loading a pre-trained English-German tokenizer.\n2. Determining the maximum sequence length for tokenized sequences.\n3. Defining a preprocessing function to tokenize the source (German) \nand target (English) sentences.\n4. Applying the preprocessing function to a subset of the training set.\n5. Converting the tokenized examples to PyTorch tensors for consistency.\n\nExample output after preprocessing:\nConverts examples from the WMT14 dataset to tokenized inputs and decoder inputs\n\n# Example\nExample 0\nInput IDs: tensor([423, 42, 11967, 2985, 2, 29, 9, 372, 4209, 8319])\nDecoder Input IDs: tensor([4677, 2, 52, 41, 73, 72, 1770, 2, 4, 48186])\nDecoded decoder input: Although, as you will have seen, the dreaded \n'millennium bug' failed to materialise...\n\n\"\"\"\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n\n# Pre-trained English-German tokenizer/model\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n\n# print(\"Tokenizer pad token:\", tokenizer.pad_token)  # prints: '<pad>'\n# print(\"Tokenizer pad token id:\", tokenizer.pad_token_id)  # prints: '58100'\n\n\n# Determine max length of tokenized sequences before padding\n# def length_check_function(examples):\n#     # Temporarily no truncation or padding\n#     src_lengths, tgt_lengths = [], []\n#     for translation in examples[\"translation\"]:\n#         src_lengths.append(len(tokenizer.tokenize(translation[\"de\"])))\n#         tgt_lengths.append(len(tokenizer.tokenize(translation[\"en\"])))\n\n#     return {\"src_length\": src_lengths, \"tgt_length\": tgt_lengths}\n\n\n# lengths = wmt14[\"train\"].map(length_check_function, batched=True)\n# max_source_length = max(lengths[\"src_length\"])\n# max_target_length = max(lengths[\"tgt_length\"])\n# max_seq_len = max(max_source_length, max_target_length)\n# print(\"Max sequence length:\", max_seq_len)  # prints: '13,614'\n\nmax_seq_len = 32\n\n\nif tokenizer.bos_token is None:\n    tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n    # If a model is already loaded, you may need:\n    model.resize_token_embeddings(len(tokenizer))\n\n\ndef preprocess_function(examples):\n    # Tokenize source (German)\n    model_inputs = tokenizer(\n        [translation[\"de\"] for translation in examples[\"translation\"]],\n        max_length=max_seq_len,\n        padding=\"max_length\",\n        truncation=True,\n    )\n\n    # Tokenize target (English)\n    with tokenizer.as_target_tokenizer():\n        tokenized_targets = tokenizer(\n            [translation[\"en\"] for translation in examples[\"translation\"]],\n            max_length=max_seq_len,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n    input_ids = tokenized_targets[\"input_ids\"]\n\n    # Ensure bos_token_id is defined (after adding bos_token if needed)\n    bos_id = tokenizer.bos_token_id\n    eos_id = tokenizer.eos_token_id\n\n    # Create decoder_input_ids by prepending BOS and removing the last token\n    # Example:\n    # original input_ids: [w1, w2, w3, eos]\n    # decoder_input_ids:  [bos_id, w1, w2, w3]\n    # labels:             [w1, w2, w3, eos]\n    decoder_input_ids = [\n        [bos_id] + [seq_id for seq_id in seq if seq_id != eos_id] for seq in input_ids\n    ]\n\n    # Update model_inputs\n    model_inputs[\"labels\"] = input_ids\n    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n\n    model_inputs.pop(\"attention_mask\", None)\n\n    return model_inputs\n\n\n# Select a subset of the training set\nwmt14_subset = wmt14[\"train\"].select(range(128))\n\n# Now apply your preprocessing function to just those 128 examples\ntokenized_wmt14_subset = wmt14_subset.map(\n    preprocess_function, batched=True, remove_columns=wmt14[\"train\"].column_names\n)\n\n# Convert to torch for consistency\ntokenized_wmt14_subset.set_format(\n    type=\"torch\", columns=[\"input_ids\", \"decoder_input_ids\", \"labels\"]\n)\n\nprint(\n    f\"Checking if the tokenized_wmt14 dataset is cached correctly: {tokenized_wmt14_subset.cache_files}\"\n)\n\n\n# Now you can print out some examples to visually inspect them\nfor i in range(3):\n    print(\"Example\", i)\n    print(\"Input IDs:\", tokenized_wmt14_subset[i][\"input_ids\"])\n    print(\"Decoder Input IDs:\", tokenized_wmt14_subset[i][\"decoder_input_ids\"])\n    print(\"Labels:\", tokenized_wmt14_subset[i][\"labels\"])\n\n    print(\n        f\"decoded_input size: {tokenized_wmt14_subset[i]['input_ids'].size()}\",\n        \"Decoded input:\",\n        tokenizer.decode(tokenized_wmt14_subset[i][\"input_ids\"]).replace(\" <pad>\", \"\"),\n    )\n    print(\n        f\"decoded_decoder_input size: {tokenized_wmt14_subset[i]['decoder_input_ids'].size()}\",\n        \"Decoded decoder input:\",\n        tokenizer.decode(tokenized_wmt14_subset[i][\"decoder_input_ids\"])\n        .replace(\" <pad>\", \"\")\n        .replace(\"<pad>\", \"\"),\n    )\n    # print(\n    #     f\"decoded_labels size: {tokenized_wmt14_subset[i]['labels'].size()}\",\n    #     \"Decoded labels:\",\n    #     tokenizer.decode(tokenized_wmt14_subset[i][\"labels\"]).replace(\" <pad>\", \"\"),\n    # )\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:31:41.128464Z","iopub.execute_input":"2024-12-11T01:31:41.129010Z","iopub.status.idle":"2024-12-11T01:31:49.501553Z","shell.execute_reply.started":"2024-12-11T01:31:41.128971Z","shell.execute_reply":"2024-12-11T01:31:49.500644Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713b758c33914f35a781d4d166940f2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fdca8c7316848a7b046872305fb3000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312656936efa43089dc544575077f4d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2344cf8c991646a087d1b0e6ffddebaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f88958f6ff74af98651df1f4ab35350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bfcf720df124e2b929ff493a698e6eb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6568c21e50747e69e6bed39116222a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38c85af6b4940b9857d2b89aa081acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/128 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c11fa8105c647b9a387553dfd0e0047"}},"metadata":{}},{"name":"stdout","text":"Checking if the tokenized_wmt14 dataset is cached correctly: [{'filename': '/root/.cache/huggingface/datasets/wmt14/de-en/0.0.0/b199e406369ec1b7634206d3ded5ba45de2fe696/cache-cf35eeca5aedf53a.arrow'}]\nExample 0\nInput IDs: tensor([35999,     9,  4371, 25478,     0, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100])\nDecoder Input IDs: tensor([58101,   465, 32932,     7,     4,  6274, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100])\nLabels: tensor([  465, 32932,     7,     4,  6274,     0, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n        58100, 58100])\ndecoded_input size: torch.Size([32]) Decoded input: Wiederaufnahme der Sitzungsperiode</s>\ndecoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> Resumption of the session\n\nExample 1\nInput IDs: tensor([  105, 34827,    11,   121,  5736,     2,    57,  4675,  1128, 25548,\n           18,  4371, 25478,    40,   320,  1396,    28,   427,  1726,  9692,\n            2, 23987,   373, 17617,   667,  8448,   123,  1926,  5806,    10,\n         6497,     0])\nDecoder Input IDs: tensor([58101,    38, 18950, 38309,     4,  6274,     7,     4,   151,   626,\n         3059, 29423,  8575,    32,  5682,   507,  1088, 17129,     8,    38,\n          206,   209,  1430,   696,    12,  2455,    41,    14,  3360,   168,\n          332,     5])\nLabels: tensor([   38, 18950, 38309,     4,  6274,     7,     4,   151,   626,  3059,\n        29423,  8575,    32,  5682,   507,  1088, 17129,     8,    38,   206,\n          209,  1430,   696,    12,  2455,    41,    14,  3360,   168,   332,\n            5,     0])\ndecoded_input size: torch.Size([32]) Decoded input: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe</s>\ndecoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in\n\nExample 2\nInput IDs: tensor([  423,    42, 11967,  2985,     2,    29,     9,   372,  4209,  8319,\n          227,    47,   387, 26050,  1540,    13,   188,  2597,    47,    51,\n        28970,     3,  2436,    70,  2435,  9449,   592,   445,  4655,    21,\n        13932,     0])\nDecoder Input IDs: tensor([58101,  4677,     2,    52,    41,    73,    72,  1770,     2,     4,\n        48186,   108,   255, 24526,    15, 44540, 19113,    22,  9326,    12,\n         1337,  2029,     2,   530,     4,   238,     5,    14,   438,     7,\n          419, 12971])\nLabels: tensor([ 4677,     2,    52,    41,    73,    72,  1770,     2,     4, 48186,\n          108,   255, 24526,    15, 44540, 19113,    22,  9326,    12,  1337,\n         2029,     2,   530,     4,   238,     5,    14,   438,     7,   419,\n        12971,     0])\ndecoded_input size: torch.Size([32]) Decoded input: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklich</s>\ndecoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.nn import CrossEntropyLoss\n# from data.tokenizer import tokenized_wmt14_subset, tokenizer\n\ndata_loader = DataLoader(tokenized_wmt14_subset, batch_size=16, shuffle=True)\nmodel = Transformer(\n    batch_size=16,\n    seq_len=32,\n    d_model=64,\n    # +1 for bos token\n    input_vocab_size=tokenizer.vocab_size + 1,\n    output_vocab_size=tokenizer.vocab_size + 1,\n    dropout_p=0.1,\n)\n\nd_model = 64\n\n\ndef lr_lambda(step_num):\n    if step_num == 0:\n        step_num = 1\n    return d_model ** (-0.5) * step_num ** (-0.5)\n\n\ntrainer_args = {\n    \"lr\": 1.0,\n    \"optimizer\": Adam,\n}\n\nscheulder = {\"scheduler\": lr_scheduler.LambdaLR, \"lr_lambda\": lr_lambda}\ntrainer = Trainer(\n    model=model, loss_func=CrossEntropyLoss(), scheduler=scheulder, **trainer_args\n)\n\ntrainer.train(\n    data_loader=data_loader, n_epochs=200, pad_token_id=tokenizer.pad_token_id\n)\n# trainer.run_one_epoch(\n#     data_loader=data_loader, verbose=True, pad_token_id=tokenizer.pad_token_id\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:57:51.197971Z","iopub.execute_input":"2024-12-10T20:57:51.198564Z","iopub.status.idle":"2024-12-10T21:25:16.912212Z","shell.execute_reply.started":"2024-12-10T20:57:51.198529Z","shell.execute_reply":"2024-12-10T21:25:16.911264Z"}},"outputs":[{"name":"stdout","text":"Epoch:     20  Loss:  0.203  in   2.7 min\nEpoch:     40  Loss:  0.149  in   5.5 min\nEpoch:     60  Loss:  0.130  in   8.2 min\nEpoch:     80  Loss:  0.118  in  10.9 min\nEpoch:    100  Loss:  0.105  in  13.6 min\nEpoch:    120  Loss:  0.098  in  16.4 min\nEpoch:    140  Loss:  0.087  in  19.2 min\nEpoch:    160  Loss:  0.062  in  21.9 min\nEpoch:    180  Loss:  0.030  in  24.7 min\nEpoch:    200  Loss:  0.008  in  27.4 min\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.5990945361554623,\n 0.3460991494357586,\n 0.2838614545762539,\n 0.2706680204719305,\n 0.2649609725922346,\n 0.26009259931743145,\n 0.2560202553868294,\n 0.25204641185700893,\n 0.24822290055453777,\n 0.24444864504039288,\n 0.24056270718574524,\n 0.2365652173757553,\n 0.23246987722814083,\n 0.22829570434987545,\n 0.2240456771105528,\n 0.2197423279285431,\n 0.21541789919137955,\n 0.21109434589743614,\n 0.20680331997573376,\n 0.20257563143968582,\n 0.19843983463943005,\n 0.19442328438162804,\n 0.19054811634123325,\n 0.1868303120136261,\n 0.18328220024704933,\n 0.17991124279797077,\n 0.1767191793769598,\n 0.17370428144931793,\n 0.17086176574230194,\n 0.168183920904994,\n 0.16566254943609238,\n 0.16328847780823708,\n 0.16105248034000397,\n 0.15894616954028606,\n 0.1569603830575943,\n 0.155086949467659,\n 0.1533187571913004,\n 0.15164792351424694,\n 0.1500674020498991,\n 0.14857107773423195,\n 0.14715373516082764,\n 0.14580910559743643,\n 0.1445323694497347,\n 0.14331951458007097,\n 0.1421651765704155,\n 0.14106592535972595,\n 0.14001734927296638,\n 0.1390166338533163,\n 0.13806091155856848,\n 0.13714505173265934,\n 0.13626705668866634,\n 0.13542548660188913,\n 0.13461766950786114,\n 0.133839868940413,\n 0.13308884017169476,\n 0.132362874224782,\n 0.13165885861963034,\n 0.1309740450233221,\n 0.13029524870216846,\n 0.12962913513183594,\n 0.12896663509309292,\n 0.1282981801778078,\n 0.1275996146723628,\n 0.12690159678459167,\n 0.12612103298306465,\n 0.12531072832643986,\n 0.1241587596014142,\n 0.12314346339553595,\n 0.1236508721485734,\n 0.1223541283980012,\n 0.12100721057504416,\n 0.12037999648600817,\n 0.11883922666311264,\n 0.1182983685284853,\n 0.11814348585903645,\n 0.12109269574284554,\n 0.12824526242911816,\n 0.12246581446379423,\n 0.1217160364612937,\n 0.11819317564368248,\n 0.11811735760420561,\n 0.12064848840236664,\n 0.11731711588799953,\n 0.1141462093219161,\n 0.11338721308857203,\n 0.11491678655147552,\n 0.11661214753985405,\n 0.11388477869331837,\n 0.11149115487933159,\n 0.10960280057042837,\n 0.10995129961520433,\n 0.11075557768344879,\n 0.11031712684780359,\n 0.10888092871755362,\n 0.10779810976237059,\n 0.10661075729876757,\n 0.10629719588905573,\n 0.10592853184789419,\n 0.10528096091002226,\n 0.10472791269421577,\n 0.10431113466620445,\n 0.10428050253540277,\n 0.10378258302807808,\n 0.10384348779916763,\n 0.10395889729261398,\n 0.10470037441700697,\n 0.10651824530214071,\n 0.10631324350833893,\n 0.10442834813147783,\n 0.1023982372134924,\n 0.1008091801777482,\n 0.1002918491140008,\n 0.09963441267609596,\n 0.0992465578019619,\n 0.0987195847555995,\n 0.0982928080484271,\n 0.09815665520727634,\n 0.09811549261212349,\n 0.09756287932395935,\n 0.0977873569354415,\n 0.09838759060949087,\n 0.09800241887569427,\n 0.09867937862873077,\n 0.1010186867788434,\n 0.1012065801769495,\n 0.09798232465982437,\n 0.09621606860309839,\n 0.09461334999650717,\n 0.09341269917786121,\n 0.09246666450053453,\n 0.09232428949326277,\n 0.09182977303862572,\n 0.09135205019265413,\n 0.09096223302185535,\n 0.09162438474595547,\n 0.09109630063176155,\n 0.0902703395113349,\n 0.09055866207927465,\n 0.09016440249979496,\n 0.08739378675818443,\n 0.08672621008008718,\n 0.0857733441516757,\n 0.08371612336486578,\n 0.0822890717536211,\n 0.08140754792839289,\n 0.08014132548123598,\n 0.07871290575712919,\n 0.07779954466968775,\n 0.07880354393273592,\n 0.07989887706935406,\n 0.07806757744401693,\n 0.075504494830966,\n 0.07305514626204967,\n 0.07055144291371107,\n 0.06923921033740044,\n 0.06752460030838847,\n 0.06595916161313653,\n 0.06421980913728476,\n 0.06303281709551811,\n 0.06239543296396732,\n 0.06292501371353865,\n 0.06393498461693525,\n 0.06557114003226161,\n 0.07405053591355681,\n 0.06799796829000115,\n 0.06045718630775809,\n 0.05379338143393397,\n 0.054469230119138956,\n 0.05271323258057237,\n 0.04962341720238328,\n 0.046746000181883574,\n 0.04389984346926212,\n 0.04246720764786005,\n 0.041110783349722624,\n 0.04030474508181214,\n 0.03794518951326609,\n 0.03576500411145389,\n 0.033651651348918676,\n 0.031651857774704695,\n 0.03021347359754145,\n 0.029361043125391006,\n 0.02814994566142559,\n 0.02620728500187397,\n 0.024884078884497285,\n 0.0237228749319911,\n 0.022495498647913337,\n 0.020719985477626324,\n 0.019619656493887305,\n 0.017745828605256975,\n 0.01616092852782458,\n 0.014954313402995467,\n 0.014311134815216064,\n 0.01312355138361454,\n 0.012137973215430975,\n 0.01125916640739888,\n 0.010412966134026647,\n 0.009566907247062773,\n 0.008793377899564803,\n 0.008131572452839464,\n 0.007726473442744464]"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using GPU to Train","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport numpy as np\n\n\ndef softmax(x: torch.Tensor, d: int):\n    return torch.nn.functional.softmax(x, dim=d)\n\n\nclass FeedForward(torch.nn.Module):\n    def __init__(\n        self,\n        d_model: int = 512,\n        hidden_size: list[tuple[int]] = None,\n        activation: torch.nn.Module = torch.nn.ReLU(),\n    ):\n        if hidden_size is None:\n            hidden_size = [(d_model, 2048), (2048, d_model)]\n\n        assert len(hidden_size) > 0, \"hidden_size must be greater than 0\"\n        assert (\n            hidden_size[0][0] == d_model and hidden_size[-1][1] == d_model\n        ), \"input and output dimensions must equal d_model\"\n\n        super().__init__()\n\n        layers = []\n        n = len(hidden_size)\n        for i in range(n):\n            layers.append(torch.nn.Linear(*hidden_size[i]))\n\n            if i < n - 1:\n                layers.append(activation)\n\n        self.net = torch.nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n\ndef attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    pad_mask: torch.Tensor,\n    mask: torch.Tensor = None,\n):\n    x = torch.matmul(q, torch.transpose(k, 1, 2))\n\n    x = x / math.sqrt(q.size(2))\n\n    if mask is not None:\n        mask = mask.to(x.device)\n        x = x + mask\n\n    pad_mask = pad_mask.to(x.device)\n    x = x + pad_mask\n\n    return torch.matmul(softmax(x, d=2), v)\n\n\nclass MultiHead(torch.nn.Module):\n    def __init__(self, h: int = 8, d_model: int = 512, mask: torch.Tensor = None):\n        assert d_model % h == 0, \"d_model must be divisible by h\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // h\n        self.d_v = self.d_k\n        self.h = h\n        self.mask = mask\n\n        self.w_q = torch.nn.Linear(d_model, d_model)\n        self.w_k = torch.nn.Linear(d_model, d_model)\n        self.w_v = torch.nn.Linear(d_model, d_model)\n        self.w_o = torch.nn.Linear(d_model, d_model)\n\n    def forward(\n        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, pad_mask: torch.Tensor\n    ):\n        batch_size, seq_len, _ = q.size()\n        q_h = self.w_q(q)\n        k_h = self.w_k(k)\n        v_h = self.w_v(v)\n\n        q_h = q_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        k_h = k_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        v_h = v_h.reshape(batch_size, seq_len, self.h, self.d_v).transpose(1, 2)\n\n        q_h = q_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        k_h = k_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        v_h = v_h.reshape(batch_size * self.h, seq_len, self.d_v)\n\n        scores = attention(q_h, k_h, v_h, pad_mask, mask=self.mask)\n\n        scores = scores.reshape(batch_size, self.h, seq_len, self.d_v).transpose(1, 2)\n        scores = scores.reshape(batch_size, seq_len, self.h * self.d_v)\n\n        return self.w_o(scores)\n\n\ndef dropout(x: torch.Tensor, p: float = 0.1):\n    assert 0 <= p <= 1, \"p must be a probability\"\n\n    if p == 1:\n        return torch.zeros_like(x)\n\n    mask = torch.full_like(x, 1 - p)\n    mask = torch.bernoulli(mask).to(x.device)\n    x = x * mask\n\n    return x * (1 / (1 - p))\n\n\nclass Transformer(torch.nn.Module):\n    def __init__(\n        self,\n        batch_size: int,\n        seq_len: int,\n        d_model: int = 512,\n        n: int = 6,\n        number_of_heads: int = 8,\n        input_vocab_size: int = 37000,\n        output_vocab_size: int = 37000,\n        dtype: torch.dtype = torch.float32,\n        dropout_p: float = None,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.seq_len = seq_len\n        self.n = n\n        self.d_model = d_model\n        self.number_of_heads = number_of_heads\n        self.vocab_size = output_vocab_size\n        self.dtype = dtype\n        self.dropout_p = dropout_p\n\n        self.register_buffer(\n            \"pe\",\n            torch.from_numpy(positional_encodings(seq_len, d_model))\n            .to(dtype)\n            .unsqueeze(0),\n        )\n        self.register_buffer(\"attention_mask\", get_attention_mask(seq_len, dtype=dtype))\n\n        self.linear = torch.nn.Linear(d_model, output_vocab_size)\n        self.input_embedding = torch.nn.Embedding(\n            num_embeddings=input_vocab_size, embedding_dim=d_model\n        )\n        self.output_embedding = torch.nn.Embedding(\n            num_embeddings=output_vocab_size, embedding_dim=d_model\n        )\n\n        self.encoders = torch.nn.ModuleList(\n            Encoder(number_of_heads=number_of_heads, d_model=d_model)\n            for _ in range(self.n)\n        )\n\n        self.decoders = torch.nn.ModuleList(\n            Decoder(\n                attention_mask=self.attention_mask,\n                number_of_heads=number_of_heads,\n                d_model=d_model,\n            )\n            for _ in range(self.n)\n        )\n\n    def forward(\n        self,\n        encoder_x: torch.Tensor,\n        decoder_x: torch.Tensor,\n        apply_softmax: bool = False,\n        pad_token_id: int = 0,\n    ):\n        encoder_x = encoder_x.to(self.pe.device)\n        decoder_x = decoder_x.to(self.pe.device)\n\n        encoder_x_pad_mask = get_pad_mask(\n            encoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n            pad_token_id=pad_token_id,\n        ).to(self.pe.device)\n        decoder_x_pad_mask = get_pad_mask(\n            decoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n            pad_token_id=pad_token_id,\n        ).to(self.pe.device)\n\n        encoder_x = (\n            self.input_embedding(encoder_x) * math.sqrt(self.d_model) + self.pe\n        )\n        decoder_x = (\n            self.output_embedding(decoder_x) * math.sqrt(self.d_model) + self.pe\n        )\n\n        for encoder in self.encoders:\n            encoder_x = encoder(encoder_x, encoder_x_pad_mask, dropout_p=self.dropout_p)\n\n        for decoder in self.decoders:\n            decoder_x = decoder(\n                decoder_x,\n                encoder_x,\n                decoder_pad_mask=decoder_x_pad_mask,\n                encoder_pad_mask=encoder_x_pad_mask,\n                dropout_p=self.dropout_p,\n            )\n\n        return (\n            softmax(self.linear(decoder_x), d=2)\n            if apply_softmax\n            else self.linear(decoder_x)\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:46:20.223003Z","iopub.execute_input":"2024-12-11T01:46:20.223358Z","iopub.status.idle":"2024-12-11T01:46:20.247624Z","shell.execute_reply.started":"2024-12-11T01:46:20.223330Z","shell.execute_reply":"2024-12-11T01:46:20.246672Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport os\n\n\ndef save(filename, **kwargs):\n    \"\"\"\n    Save a pytorch object to file\n    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n    Arguments:\n        filename: the file in which to save the object\n\n    Possible keyword arguments (kwargs):\n        epoch: the epoch so far if training\n        model_state_dict: a model's state\n        opt_state_dict: an optimizer's state, if training\n    \"\"\"\n    msg = f\"{filename} exists: delete it first to replace it.\"\n    assert not os.path.exists(filename), msg\n    torch.save(kwargs, filename)\n\n\ndef load(filename):\n    \"\"\"\n    Load a pytorch object from a given filename\n    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n    Arguments:\n        filename: the file from which to load the object\n    \"\"\"\n    return torch.load(filename)\n\n\nclass Trainer:\n    def __init__(self, optimizer, model, loss_func, scheduler=None, device=\"cpu\", **kwargs):\n        \"\"\"\n        Initialize the optimizer for the model, using any necessary kwargs\n        Save the model and loss function for later calculation\n        \"\"\"\n        self.device = device\n        self.optimizer = optimizer(model.parameters(), **kwargs)\n        self.scheduler = scheduler[\"scheduler\"](\n            self.optimizer, lr_lambda=scheduler[\"lr_lambda\"]\n        ) if scheduler else None\n        self.model = model.to(self.device)\n        self.loss_func = loss_func\n        self.epoch = 0\n        self.start_time = None\n\n    def run_one_batch(self, encoder_x, decoder_x, y, train=True, pad_token_id=0):\n        \"\"\"\n        Run self.model on one batch of data, using `self.loss_func` to compute the model's loss.\n        \"\"\"\n        # Move data to device\n        encoder_x = encoder_x.to(self.device)\n        decoder_x = decoder_x.to(self.device)\n        y = y.to(self.device)\n\n        if train:\n            self.optimizer.zero_grad()\n\n        outputs = self.model(encoder_x, decoder_x, pad_token_id=pad_token_id)\n        loss = self.loss_func(outputs.transpose(1, 2), y)\n\n        if train:\n            loss.backward()\n            self.optimizer.step()\n            if self.scheduler:\n                self.scheduler.step()\n\n        return loss.detach().cpu().numpy()\n\n    def run_one_epoch(self, data_loader, train=True, verbose=False, pad_token_id=0):\n        \"\"\"\n        Train one epoch, a batch at a time, using self.run_one_batch.\n        \"\"\"\n        np.random.seed(0)\n        torch.manual_seed(0)\n\n        if self.start_time is None:\n            self.start_time = time.time()\n\n        epoch_size = 0\n        total_loss = 0\n        for batch_idx, batch_data in enumerate(data_loader):\n            encoder_x, decoder_x, y = (\n                batch_data[\"input_ids\"],\n                batch_data[\"decoder_input_ids\"],\n                batch_data[\"labels\"],\n            )\n            epoch_size += encoder_x.size(0)\n            loss = self.run_one_batch(\n                encoder_x, decoder_x, y, train=train, pad_token_id=pad_token_id\n            )\n            total_loss += loss\n\n        avg_loss = total_loss / epoch_size\n\n        if verbose:\n            epoch = self.epoch + 1\n            duration = (time.time() - self.start_time) / 60\n            log = [f\"Epoch: {epoch:6d}\" if train else \"Eval:\" + \" \" * 8]\n            log.extend(\n                [\n                    f\"Loss: {avg_loss:6.3f}\",\n                    f\"in {duration:5.1f} min\",\n                ]\n            )\n            print(\"  \".join(log))\n\n        return avg_loss\n\n    def train(self, data_loader, n_epochs, train=True, report_every=None, pad_token_id=0):\n        \"\"\"\n        Run the model for `n_epochs` epochs on the data in `data_loader`.\n        \"\"\"\n        self.start_time = time.time()\n\n        if report_every is None:\n            report_every = max(1, n_epochs // 10)\n\n        losses = []\n        for i in range(n_epochs):\n            verbose = ((i + 1) % report_every) == 0\n            loss = self.run_one_epoch(\n                data_loader, train=train, verbose=verbose, pad_token_id=pad_token_id\n            )\n            losses.append(loss)\n            if train:\n                self.epoch += 1\n\n        return losses\n\n    def eval(self, data_loader):\n        \"\"\"\n        Helper function to run through the data loader once and just compute the loss.\n        \"\"\"\n        return self.train(data_loader, 1, train=False, report_every=1)\n\n    def save_trainer(self, filename):\n        \"\"\"\n        Save this Trainer to file.\n        \"\"\"\n        save(\n            filename,\n            epoch=self.epoch,\n            model_state_dict=self.model.state_dict(),\n            optimizer_state_dict=self.optimizer.state_dict(),\n        )\n\n    def load_trainer(self, filename):\n        \"\"\"\n        Load this Trainer from file.\n        \"\"\"\n        checkpoint = load(filename)\n        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        self.epoch = checkpoint[\"epoch\"]\n        self.model.to(self.device)\n        self.model.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:32:01.233745Z","iopub.execute_input":"2024-12-11T01:32:01.234738Z","iopub.status.idle":"2024-12-11T01:32:01.249775Z","shell.execute_reply.started":"2024-12-11T01:32:01.234698Z","shell.execute_reply":"2024-12-11T01:32:01.249078Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.nn import CrossEntropyLoss\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create DataLoader\ndata_loader = DataLoader(tokenized_wmt14_subset, batch_size=16, shuffle=True)\n\n# Initialize the model and move it to the appropriate device\nmodel = Transformer(\n    batch_size=16,\n    seq_len=32,\n    d_model=64,\n    # +1 for bos token\n    input_vocab_size=tokenizer.vocab_size + 1,\n    output_vocab_size=tokenizer.vocab_size + 1,\n    dropout_p=0.1,\n).to(device)  # Move model to device\n\n# Model dimensions\nd_model = 64\n\n# Learning rate lambda function\ndef lr_lambda(step_num):\n    if step_num == 0:\n        step_num = 1\n    return d_model ** (-0.5) * step_num ** (-0.5)\n\n# Trainer arguments\ntrainer_args = {\n    \"lr\": 1.0,\n    \"optimizer\": Adam,\n}\n\n# Learning rate scheduler\nscheduler = {\"scheduler\": lr_scheduler.LambdaLR, \"lr_lambda\": lr_lambda}\n\n# Initialize the Trainer with the device\ntrainer = Trainer(\n    model=model,\n    loss_func=CrossEntropyLoss(),\n    scheduler=scheduler,\n    device=device,  # Pass the device to Trainer\n    **trainer_args,\n)\n\n# Start training\ntrainer.train(\n    data_loader=data_loader,\n    n_epochs=200,\n    pad_token_id=tokenizer.pad_token_id,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T01:46:25.069252Z","iopub.execute_input":"2024-12-11T01:46:25.069598Z","iopub.status.idle":"2024-12-11T01:48:23.174573Z","shell.execute_reply.started":"2024-12-11T01:46:25.069570Z","shell.execute_reply":"2024-12-11T01:48:23.173448Z"}},"outputs":[{"name":"stdout","text":"Epoch:     20  Loss:  0.273  in   0.2 min\nEpoch:     40  Loss:  0.273  in   0.4 min\nEpoch:     60  Loss:  0.273  in   0.6 min\nEpoch:     80  Loss:  0.272  in   0.8 min\nEpoch:    100  Loss:  0.272  in   1.0 min\nEpoch:    120  Loss:  0.269  in   1.2 min\nEpoch:    140  Loss:  0.263  in   1.4 min\nEpoch:    160  Loss:  0.248  in   1.6 min\nEpoch:    180  Loss:  0.184  in   1.8 min\nEpoch:    200  Loss:  0.121  in   2.0 min\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[0.5565645806491375,\n 0.3105028010904789,\n 0.28261831775307655,\n 0.27772708237171173,\n 0.27665937319397926,\n 0.27443154342472553,\n 0.2741149440407753,\n 0.2738286331295967,\n 0.2736268937587738,\n 0.27355015464127064,\n 0.273493692278862,\n 0.2734227627515793,\n 0.27335039898753166,\n 0.2733312267810106,\n 0.27330354787409306,\n 0.2732666824012995,\n 0.27324438467621803,\n 0.2732210773974657,\n 0.27319613099098206,\n 0.2731740493327379,\n 0.27315371856093407,\n 0.27313463762402534,\n 0.27311631850898266,\n 0.2730988934636116,\n 0.2730824612081051,\n 0.2730666249990463,\n 0.27305140905082226,\n 0.2730368487536907,\n 0.27302277088165283,\n 0.273009130731225,\n 0.27299596555531025,\n 0.272983193397522,\n 0.27297077141702175,\n 0.2729587461799383,\n 0.272946959361434,\n 0.2729355115443468,\n 0.27292428724467754,\n 0.27291329950094223,\n 0.2729025688022375,\n 0.27289201505482197,\n 0.272881630808115,\n 0.2728713620454073,\n 0.27286130748689175,\n 0.2728513702750206,\n 0.27284157276153564,\n 0.2728318851441145,\n 0.27282232604920864,\n 0.27281283028423786,\n 0.27280337549746037,\n 0.2727940008044243,\n 0.27278469502925873,\n 0.2727754022926092,\n 0.27276616357266903,\n 0.27275687642395496,\n 0.27274768613278866,\n 0.2727384641766548,\n 0.27272922918200493,\n 0.27271995320916176,\n 0.2727106399834156,\n 0.27270129695534706,\n 0.2726918626576662,\n 0.27268239855766296,\n 0.27267282642424107,\n 0.27266316674649715,\n 0.27265341207385063,\n 0.2726435475051403,\n 0.2726334948092699,\n 0.2726233210414648,\n 0.2726129870861769,\n 0.2726024463772774,\n 0.27259170822799206,\n 0.27258074656128883,\n 0.27256953716278076,\n 0.27255819365382195,\n 0.2725465390831232,\n 0.2725346013903618,\n 0.27252235636115074,\n 0.2725097220391035,\n 0.2724968157708645,\n 0.27248349972069263,\n 0.27246973663568497,\n 0.27245547622442245,\n 0.272440729662776,\n 0.27242546901106834,\n 0.27240949869155884,\n 0.27239287458360195,\n 0.2723756264895201,\n 0.2723575718700886,\n 0.2723387572914362,\n 0.2723190076649189,\n 0.27229825779795647,\n 0.2722764629870653,\n 0.27225345745682716,\n 0.27222921699285507,\n 0.2722034230828285,\n 0.2721760142594576,\n 0.27214695140719414,\n 0.2721160352230072,\n 0.2720829267054796,\n 0.27204760909080505,\n 0.2720096502453089,\n 0.2719686795026064,\n 0.27192438021302223,\n 0.2718762047588825,\n 0.2718238867819309,\n 0.2717664781957865,\n 0.2717033736407757,\n 0.27163373678922653,\n 0.27155651710927486,\n 0.2714702859520912,\n 0.2713734433054924,\n 0.27126423828303814,\n 0.27114024572074413,\n 0.2709986139088869,\n 0.2708351816982031,\n 0.27064542286098003,\n 0.27042209915816784,\n 0.27015776187181473,\n 0.26984354108572006,\n 0.26946985721588135,\n 0.2690344378352165,\n 0.2685382105410099,\n 0.26800588704645634,\n 0.2674807123839855,\n 0.26701793633401394,\n 0.26663545332849026,\n 0.2663087956607342,\n 0.26601025834679604,\n 0.2657248042523861,\n 0.2654510624706745,\n 0.26522515155375004,\n 0.26507505401968956,\n 0.26501287892460823,\n 0.26503947749733925,\n 0.2651008926331997,\n 0.2648517806082964,\n 0.2647769507020712,\n 0.26422765851020813,\n 0.2634987533092499,\n 0.26311869733035564,\n 0.2628507036715746,\n 0.26259912364184856,\n 0.2623283173888922,\n 0.2620593514293432,\n 0.26182370632886887,\n 0.2615674398839474,\n 0.2612579744309187,\n 0.2608237713575363,\n 0.2602962665259838,\n 0.25980282202363014,\n 0.2593591697514057,\n 0.258782222867012,\n 0.2579312436282635,\n 0.2568620890378952,\n 0.25584610365331173,\n 0.2547940742224455,\n 0.2536060679703951,\n 0.25184958986938,\n 0.249734653159976,\n 0.2478786613792181,\n 0.2458512894809246,\n 0.2430633082985878,\n 0.24055424891412258,\n 0.23750911466777325,\n 0.23487099818885326,\n 0.2327613364905119,\n 0.23127394169569016,\n 0.22912375070154667,\n 0.22575854696333408,\n 0.22168018482625484,\n 0.21762116253376007,\n 0.2144188228994608,\n 0.21174699999392033,\n 0.20735319145023823,\n 0.2026956006884575,\n 0.1994502693414688,\n 0.19617333449423313,\n 0.1924738958477974,\n 0.18806923739612103,\n 0.18373660929501057,\n 0.178678585216403,\n 0.17356025241315365,\n 0.16927527263760567,\n 0.1642664186656475,\n 0.16045614518225193,\n 0.15600611828267574,\n 0.15258103981614113,\n 0.14947024919092655,\n 0.14610129036009312,\n 0.14351573027670383,\n 0.13954598736017942,\n 0.13732781447470188,\n 0.13568083755671978,\n 0.13320788089185953,\n 0.13182589411735535,\n 0.12965636886656284,\n 0.12767564970999956,\n 0.12268298864364624,\n 0.12217827327549458,\n 0.12069785129278898]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}