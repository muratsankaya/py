{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:27:58.541655Z","iopub.execute_input":"2024-12-07T20:27:58.542000Z","iopub.status.idle":"2024-12-07T20:27:58.903581Z","shell.execute_reply.started":"2024-12-07T20:27:58.541969Z","shell.execute_reply":"2024-12-07T20:27:58.902598Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install datasets\n!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:28:00.567150Z","iopub.execute_input":"2024-12-07T20:28:00.567600Z","iopub.status.idle":"2024-12-07T20:28:19.685000Z","shell.execute_reply.started":"2024-12-07T20:28:00.567542Z","shell.execute_reply":"2024-12-07T20:28:19.684110Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.0.0 sacrebleu-2.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:19:58.341100Z","iopub.execute_input":"2024-11-24T19:19:58.341593Z","iopub.status.idle":"2024-11-24T19:19:59.570146Z","shell.execute_reply.started":"2024-11-24T19:19:58.341536Z","shell.execute_reply":"2024-11-24T19:19:59.568854Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wmt14\", \"de-en\")  # WMT-14 English-German\ntrain_data = dataset[\"train\"]\nvalid_data = dataset[\"validation\"]\ntest_data = dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:20:02.627727Z","iopub.execute_input":"2024-11-24T19:20:02.628344Z","iopub.status.idle":"2024-11-24T19:20:26.762498Z","shell.execute_reply.started":"2024-11-24T19:20:02.628305Z","shell.execute_reply":"2024-11-24T19:20:26.761229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d8d64f3814440cba5474406e1678cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa2ae5e79b394c55ac5ea9cfd047ef83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f47d6f00ca4e1d841002feebbdd8c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85fab23d3274d86ad7e9cf85dd624eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a23986f492434a9173a54c17afcd99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73f582a8d22f4b0b9632b0c50a61c6a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b113e17de6b5492e92b8dc779f6be883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7bb13eef3a4db79c5c6f343ebcf5f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6745a36e434b60941ba1f181ff3be6"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:46:33.082161Z","iopub.execute_input":"2024-11-24T19:46:33.082552Z","iopub.status.idle":"2024-11-24T19:46:33.089756Z","shell.execute_reply.started":"2024-11-24T19:46:33.082517Z","shell.execute_reply":"2024-11-24T19:46:33.088459Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['translation'],\n    num_rows: 10\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\n# Initialize a BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n\n# Extract English and German sentences\ncorpus = [example[\"translation\"][\"en\"] for example in train_data] + \\\n         [example[\"translation\"][\"de\"] for example in train_data]\n\n# Train the tokenizer on the corpus\ntrainer = trainers.BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"], vocab_size=32000)\ntokenizer.train_from_iterator(corpus, trainer=trainer)\n\n# Save the tokenizer for future use\ntokenizer.save(\"bpe_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:56:57.876080Z","iopub.execute_input":"2024-11-24T21:56:57.876876Z","iopub.status.idle":"2024-11-24T21:56:58.052769Z","shell.execute_reply.started":"2024-11-24T21:56:57.876817Z","shell.execute_reply":"2024-11-24T21:56:58.051216Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def tokenize_function(example):\n    # Tokenize English and German text\n    src_tokens = tokenizer.encode(example[\"translation\"][\"en\"]).ids\n    tgt_tokens = tokenizer.encode(example[\"translation\"][\"de\"]).ids\n    return {\"src_tokens\": src_tokens, \"tgt_tokens\": tgt_tokens}\n\n# Apply tokenization\ntrain_data = train_data.map(tokenize_function, remove_columns=[\"translation\"])\n# valid_data = valid_data.map(tokenize_function, remove_columns=[\"translation\"])\n# test_data = test_data.map(tokenize_function, remove_columns=[\"translation\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:57:16.140948Z","iopub.execute_input":"2024-11-24T21:57:16.141334Z","iopub.status.idle":"2024-11-24T21:57:16.205829Z","shell.execute_reply.started":"2024-11-24T21:57:16.141301Z","shell.execute_reply":"2024-11-24T21:57:16.204384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"498cabee086946a4a8e40fcb1bfe5897"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:57:28.023769Z","iopub.execute_input":"2024-11-24T21:57:28.024215Z","iopub.status.idle":"2024-11-24T21:57:28.032273Z","shell.execute_reply.started":"2024-11-24T21:57:28.024178Z","shell.execute_reply":"2024-11-24T21:57:28.030978Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['src', 'tgt', 'src_tokens', 'tgt_tokens'],\n    num_rows: 10\n})"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\n# Define a data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n\n# Create PyTorch DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=data_collator)\nvalid_loader = DataLoader(valid_data, batch_size=32, collate_fn=data_collator)\ntest_loader = DataLoader(test_data, batch_size=32, collate_fn=data_collator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:58:18.159516Z","iopub.execute_input":"2024-11-24T21:58:18.159986Z","iopub.status.idle":"2024-11-24T21:58:43.320992Z","shell.execute_reply.started":"2024-11-24T21:58:18.159941Z","shell.execute_reply":"2024-11-24T21:58:43.319602Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a6a19069504201a4014e3ca10fda6e"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"train_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:59:04.621112Z","iopub.execute_input":"2024-11-24T21:59:04.621537Z","iopub.status.idle":"2024-11-24T21:59:04.629127Z","shell.execute_reply.started":"2024-11-24T21:59:04.621495Z","shell.execute_reply":"2024-11-24T21:59:04.627938Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x790836d3fb20>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import torch.nn as nn\n\n# Define the embedding layer\nvocab_size = tokenizer.get_vocab_size()\nd_model = 512  # Dimension of embeddings\nembedding = nn.Embedding(vocab_size, d_model)\n\n# Example usage\nbatch = next(iter(train_loader))  # A batch from the dataloader\nsrc_tokens = batch[\"src_tokens\"]  # Source token IDs\ntgt_tokens = batch[\"tgt_tokens\"]  # Target token IDs\n\nsrc_embeddings = embedding(src_tokens)  # Shape: (batch_size, seq_length, d_model)\ntgt_embeddings = embedding(tgt_tokens)  # Shape: (batch_size, seq_length, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T21:59:26.955099Z","iopub.execute_input":"2024-11-24T21:59:26.955635Z","iopub.status.idle":"2024-11-24T21:59:27.656033Z","shell.execute_reply.started":"2024-11-24T21:59:26.955548Z","shell.execute_reply":"2024-11-24T21:59:27.654315Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# A batch from the dataloader\u001b[39;00m\n\u001b[1;32m     10\u001b[0m src_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Source token IDs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tgt_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Target token IDs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'pad'"],"ename":"AttributeError","evalue":"'tokenizers.Tokenizer' object has no attribute 'pad'","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport torch\nimport numpy as np\n\n\ndef softmax(x: torch.Tensor, d: int):\n    # return torch.nn.Softmax(dim=d)(x)\n    return torch.nn.functional.softmax(x, dim=d)\n\n\nclass FeedForward(torch.nn.Module):\n    def __init__(\n        self,\n        d_model: int = 512,\n        hidden_size: list[tuple[int]] = None,\n        activation: torch.nn.Module = torch.nn.ReLU(),\n    ):\n        \"\"\"\n        notes:\n            - default dimensions are set according to paper\n\n        \"\"\"\n        if hidden_size is None:\n            hidden_size = [(d_model, 2048), (2048, d_model)]\n\n        assert len(hidden_size) > 0, \"hidden_size must be greater than 0\"\n        assert (\n            hidden_size[0][0] == d_model and hidden_size[-1][1] == d_model\n        ), \"input and output dimensions must equal d_model\"\n\n        super().__init__()\n\n        layers = []\n        n = len(hidden_size)\n        for i in range(n):\n            layers.append(torch.nn.Linear(*hidden_size[i]))\n\n            # No activation after the final layer\n            if i < n - 1:\n                layers.append(activation)\n\n        self.net = torch.nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n\ndef attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    pad_mask: torch.Tensor,\n    mask: torch.Tensor = None,\n):\n    \"\"\"\n    input:\n        q: a torch tensor of size: (batch_size*h, seq_len, d_k)\n        k: a torch tensor of size: (batch_size*h, seq_len, d_k)\n        v: a torch tensor of size: (batch_size*h, seq_len, d_v)\n        pad_mask: a torch tensor of size: (batch_size*h, seq_len, seq_len)\n        mask: a torch tensor of size: (seq_len, seq_len)\n\n    output:\n        A torch tensor of size: (batch_size*h, seq_len, d_v)\n\n    \"\"\"\n    x = torch.matmul(q, torch.transpose(k, 1, 2))\n\n    # Scale x by sqrt(d_k)\n    x = x / math.sqrt(q.size(2))\n\n    if mask is not None:\n\n        # Broadcasting will match the size at dim=0\n        mask = mask.unsqueeze(0)\n\n        # Apply mask via element-wise addition\n        x = x + mask\n\n    x = x + pad_mask\n\n    return torch.matmul(softmax(x, d=2), v)\n\n\nclass MultiHead(torch.nn.Module):\n    def __init__(self, h: int = 8, d_model: int = 512, mask: torch.Tensor = None):\n        \"\"\"\n        input:\n            h: number of heads\n            d_model: model dimensions, i.e. embedding size\n            mask: a boolean to apply masked multi-head attention\n        notes:\n            - default dimensions are set according to the paper\n\n        \"\"\"\n        assert d_model % h == 0, \"d_model must be divisible by h\"\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // h\n        self.d_v = self.d_k\n        self.h = h\n        self.mask = mask\n\n        self.w_q = torch.nn.Linear(d_model, d_model)\n        self.w_k = torch.nn.Linear(d_model, d_model)\n        self.w_v = torch.nn.Linear(d_model, d_model)\n        self.w_o = torch.nn.Linear(d_model, d_model)\n\n    def forward(\n        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, pad_mask: torch.Tensor\n    ):\n        \"\"\"\n        input:\n            q: a torch tensor of size: (batch_size, seq_len, d_model)\n            k: a torch tensor of size: (batch_size, seq_len, d_model)\n            v: a torch tensor of size: (batch_size, seq_len, d_model)\n            pad_mask: a torch tensor of size: (batch_size, seq_len, seq_len)\n\n        output:\n            A torch tensor of size: (batch_size, seq_len, d_model)\n\n        notes:\n            - d_model is essentially embedding dimensions\n        \"\"\"\n        batch_size, seq_len, _ = q.size()\n        q_h = self.w_q(q)\n        k_h = self.w_k(k)\n        v_h = self.w_v(v)\n\n        # Splitting q, k and v tensors in to h heads\n        q_h = q_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        k_h = k_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n        v_h = v_h.reshape(batch_size, seq_len, self.h, self.d_v).transpose(1, 2)\n\n        # Combine heads for parrallel computation\n        q_h = q_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        k_h = k_h.reshape(batch_size * self.h, seq_len, self.d_k)\n        v_h = v_h.reshape(batch_size * self.h, seq_len, self.d_v)\n\n        # Apply attention\n        scores = attention(q_h, k_h, v_h, pad_mask, mask=self.mask)\n\n        # Seperate heads\n        scores = scores.reshape(batch_size, self.h, seq_len, self.d_v).transpose(1, 2)\n\n        # Concat h heads (Concat(head1, ..., headh))\n        scores = scores.reshape(batch_size, seq_len, self.h * self.d_v)\n\n        return self.w_o(scores)\n\n\n# TODO: can change the definition so that if dropout called with\n# p=None then can just terminate. This would make the Encoder and\n# Decoder forward passes more concise\ndef dropout(x: torch.Tensor, p: float = 0.1):\n    assert 0 <= p <= 1, \"p must be a probability\"\n\n    if p == 1:\n        # All elements are dropped; just return zeros.\n        return torch.zeros_like(x)\n\n    # Create a tensor with the same shape as x\n    # and set all is values to 1 - p\n    mask = torch.full_like(x, 1 - p)\n\n    # Will sample the entries from the bernoulli distribution.\n    # The i'th entry of the output tensor will draw a value 1 according\n    # to the i'th probability given the input tensor.\n    mask = torch.bernoulli(mask).to(x.device)\n\n    # Apply dropout via element-wise multiplication.\n    x = x * mask\n\n    # Apply inverted scaling\n    return x * (1 / (1 - p))\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, number_of_heads: int = 8, d_model: int = 512):\n        \"\"\"\n        notes:\n            - default parameter values are based on the paper\n        \"\"\"\n        assert (\n            d_model % number_of_heads == 0\n        ), \"d_model must be divisible by number_of_heads\"\n        super().__init__()\n        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n        self.feed_forward = FeedForward(d_model=d_model)\n        self.d_model = d_model\n        self.layer_norms = torch.nn.ModuleList(\n            torch.nn.LayerNorm(d_model) for _ in range(2)\n        )\n\n    def forward(\n        self, x: torch.Tensor, pad_mask: torch.Tensor, dropout_p=None\n    ) -> torch.Tensor:\n        \"\"\"\n        input:\n            x: a torch tensor of size: (batch_size, seq_len, d_model)\n        output:\n            a torch tensor of size: (batch_size, seq_len, d_model)\n        \"\"\"\n\n        x = self.layer_norms[0](\n            x\n            + (\n                self.multi_head_attention(x, x, x, pad_mask)\n                if dropout_p is None\n                else dropout(self.multi_head_attention(x, x, x, pad_mask), p=dropout_p)\n            )\n        )\n\n        return self.layer_norms[1](\n            x\n            + (\n                self.feed_forward(x)\n                if dropout_p is None\n                else dropout(self.feed_forward(x), p=dropout_p)\n            )\n        )\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(\n        self,\n        attention_mask: torch.Tensor,\n        number_of_heads: int = 8,\n        d_model: int = 512,\n    ):\n        \"\"\"\n        notes:\n            - default parameter values are based on the paper\n            - in the previous implementation we were using the same multi head\n            module and inidicating the apply mask on the forward pass. That's an\n            outregous mistake. The same parameters are being used in that case\n            only when applying attention part of the sequence was getting masked.\n        \"\"\"\n        assert (\n            d_model % number_of_heads == 0\n        ), \"d_model must be divisible by number_of_heads\"\n        super().__init__()\n        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n        self.masked_multi_head_attention = MultiHead(\n            h=number_of_heads, d_model=d_model, mask=attention_mask\n        )\n        self.feed_forward = FeedForward(d_model=d_model)\n        self.d_model = d_model\n        self.layer_norms = torch.nn.ModuleList(\n            torch.nn.LayerNorm(d_model) for _ in range(3)\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        encoder_x: torch.Tensor,\n        decoder_pad_mask: torch.Tensor,\n        encoder_pad_mask: torch.Tensor,\n        dropout_p=None,\n    ) -> torch.Tensor:\n        \"\"\"\n        inputs:\n            x: a torch tensor of size: (batch_size, seq_len, d_model)\n            encoder_x: a torch tensor of size: (batch_size, seq_len, d_model)\n        output:\n            a torch tensor of size: (batch_size, seq_len, d_model)\n        \"\"\"\n        x = self.layer_norms[0](\n            x\n            + (\n                self.masked_multi_head_attention(x, x, x, decoder_pad_mask)\n                if dropout_p is None\n                else dropout(\n                    self.masked_multi_head_attention(x, x, x, decoder_pad_mask),\n                    p=dropout_p,\n                )\n            )\n        )\n\n        # Cross-Attention\n        # Here encoder_pad_mask must be used because\n        x = self.layer_norms[1](\n            x\n            + (\n                self.multi_head_attention(x, encoder_x, encoder_x, encoder_pad_mask)\n                if dropout_p is None\n                else dropout(\n                    self.multi_head_attention(\n                        x, encoder_x, encoder_x, encoder_pad_mask\n                    ),\n                    p=dropout_p,\n                )\n            )\n        )\n\n        return self.layer_norms[2](\n            x\n            + (\n                self.feed_forward(x)\n                if dropout_p is None\n                else dropout(self.feed_forward(x), p=dropout_p)\n            )\n        )\n\n\ndef positional_encodings(seq_len: int, d_model: int) -> np.ndarray:\n    pos, i = np.indices((seq_len, d_model))\n    return np.where(\n        i % 2 == 0,\n        np.sin(pos / np.power(10000, (2 * i / d_model))),\n        np.cos(pos / np.power(10000, (2 * i / d_model))),\n    )\n\n\ndef get_attention_mask(seq_len: int, dtype: torch.dtype) -> torch.Tensor:\n    \"\"\"\n    Here we will be creating a triangular matrix where\n    all the upper triangle (above the diagonal) is set to -oo.\n\n    output:\n        A torch tensor of size: (seq_len, seq_len)\n    \"\"\"\n\n    mask_x = torch.full((seq_len, seq_len), float(\"-inf\"), dtype=dtype)\n\n    return torch.triu(mask_x, diagonal=1)\n\n    # Add a new batch_size dimension and expand it to\n    # match batch_size\n    # -1 means keep the size at that dimension\n    # However its also documented here:\n    # https://stackoverflow.com/questions/65900110/does-pytorch-broadcast-consume-less-memory-than-expand\n    # that expand does not also consume extra memory\n    # mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n\n\ndef get_pad_mask(x: torch.Tensor, h, batch_size, seq_len) -> torch.Tensor:\n    \"\"\"\n    inputs:\n        x: a torch tensor of size: (batch_size, seq_len)\n    \"\"\"\n    # 0 must be the padding token id\n    # Here torch.where(encoder_x == 0, float(\"-inf\"), 0)\n    #    .unsqueeze(2)\n    #    .expand(-1, -1, self.seq-len)\n    # should also mathematically produce the same shape\n    # but typically the keys gets masked so the current approach\n    # aligns better with the paper\n    assert x.size() == (\n        batch_size,\n        seq_len,\n    ), f\"x must have size: {(batch_size, seq_len)}\"\n\n    return (\n        torch.where(x == 0, float(\"-inf\"), 0)\n        .unsqueeze(1)\n        .unsqueeze(1)\n        .expand(-1, h, seq_len, -1)\n        .reshape(batch_size * h, seq_len, seq_len)\n    )\n\n\nclass Transformer(torch.nn.Module):\n    def __init__(\n        self,\n        batch_size: int,\n        seq_len: int,\n        d_model: int = 512,\n        n: int = 6,\n        number_of_heads: int = 8,\n        input_vocab_size: int = 37000,\n        output_vocab_size: int = 37000,\n        dtype: torch.dtype = torch.float32,\n    ):\n        \"\"\"\n        inputs:\n            n: the number of encoder and decoder stacks\n            d_model: model dimensions, i.e. embedding dimensions\n            number_of_heads: number of heads using in multi-head attention\n            input_vocab_size: size of the vocabulary for the input structure\n            output_vocab_size: size of the vocabulary for the output structure\n\n        notes:\n            - I'm using a fixed seq_len for both the input and the output. That could\n            be adjusted to make it varied and more flexible.\n\n        \"\"\"\n        super().__init__()\n        self.batch_size = batch_size\n        self.seq_len = seq_len\n        self.n = n\n        self.d_model = d_model\n        self.number_of_heads = number_of_heads\n        self.vocab_size = output_vocab_size\n        self.dtype = dtype\n        self.register_buffer(\n            \"pe\",\n            torch.from_numpy(positional_encodings(seq_len, d_model))\n            .to(dtype)\n            .unsqueeze(0),\n        )\n        self.register_buffer(\"attention_mask\", get_attention_mask(seq_len, dtype=dtype))\n        self.linear = torch.nn.Linear(d_model, output_vocab_size)\n\n        self.input_embedding = torch.nn.Embedding(\n            num_embeddings=input_vocab_size, embedding_dim=d_model\n        )\n        self.output_embedding = torch.nn.Embedding(\n            num_embeddings=output_vocab_size, embedding_dim=d_model\n        )\n\n        # Using ModuleList is crucial here instead of python list\n        # If python list is used, then model.parameters() will not\n        # return the paremeters of layers inside.\n        self.encoders = torch.nn.ModuleList(\n            Encoder(number_of_heads=number_of_heads, d_model=d_model)\n            for _ in range(self.n)\n        )\n\n        self.decoders = torch.nn.ModuleList(\n            Decoder(\n                attention_mask=self.attention_mask,\n                number_of_heads=number_of_heads,\n                d_model=d_model,\n            )\n            for _ in range(self.n)\n        )\n\n    def forward(\n        self,\n        encoder_x: torch.Tensor,\n        decoder_x: torch.Tensor,\n        apply_softmax: bool = False,\n    ):\n        \"\"\"\n        inputs:\n            encoder_x: a torch tensor of size: (batch_size, seq_len)\n            decoder_x: a torch tensor of size: (batch_size, seq_len)\n            apply_softmax: a boolean.\n                - Most torch loss functions expect logits instead of porbabilities.\n                So make sure that the loss function does not normalize inputs and\n                expects probabilities before setting this to True.\n\n        outputs:\n            a probability distribution over the vocabulary\n        \"\"\"\n        assert (\n            encoder_x.size() == decoder_x.size() == (self.batch_size, self.seq_len)\n        ), f\"encoder_x and decoder_x must both have the size: ({self.batch_size}, {self.seq_len})\"\n\n        assert torch.any(encoder_x) and torch.any(decoder_x), (\n            \"empty examples are not allowed. There could be some additional reasons \"\n            \"for not to allow them but simple example why is that softmax is not defined \"\n            \"over an empty sequence\"\n        )\n\n        encoder_x_pad_mask = get_pad_mask(\n            encoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n        )\n        decoder_x_pad_mask = get_pad_mask(\n            decoder_x,\n            h=self.number_of_heads,\n            batch_size=self.batch_size,\n            seq_len=self.seq_len,\n        )\n\n        # In the paper it is mentioned that they scale the embedding weights by math.sqrt(self.d_model)\n        # see the end of section 3.4 for more detail\n        encoder_x = self.input_embedding(encoder_x) * math.sqrt(self.d_model) + self.pe\n        decoder_x = self.output_embedding(decoder_x) * math.sqrt(self.d_model) + self.pe\n\n        for encoder in self.encoders:\n            encoder_x = encoder(encoder_x, encoder_x_pad_mask)\n\n        for decoder in self.decoders:\n            decoder_x = decoder(\n                decoder_x,\n                encoder_x,\n                decoder_pad_mask=decoder_x_pad_mask,\n                encoder_pad_mask=encoder_x_pad_mask,\n            )\n\n        return (\n            softmax(self.linear(decoder_x), d=2)\n            if apply_softmax\n            else self.linear(decoder_x)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:02:07.062997Z","iopub.execute_input":"2024-12-08T20:02:07.063401Z","iopub.status.idle":"2024-12-08T20:02:10.716193Z","shell.execute_reply.started":"2024-12-08T20:02:07.063371Z","shell.execute_reply":"2024-12-08T20:02:10.715372Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Use the CountingDataset\nclass CountingDataset(torch.utils.data.Dataset):\n    def __init__(self, n, max_length=8, vocab_size=3):\n        assert vocab_size > 2\n        self.n = n\n        self.vocab_size = vocab_size\n        seq_lengths = np.random.randint(max_length // 2, max_length, n)\n        data = np.random.randint(1, vocab_size, (n, max_length))\n\n        # Replace elements past the sequence length with 0 (padding token)\n        for i in range(n):\n            data[i, seq_lengths[i]:] = 0\n\n        # Labels: whether `1`s outnumber `2`s\n        num_ones = (data == 1).sum(axis=1)\n        num_twos = (data == 2).sum(axis=1)\n        self.data = torch.tensor(data, dtype=torch.long)\n        self.labels = torch.tensor((num_ones > num_twos).astype(int), dtype=torch.long)\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\n# Create dataset and dataloader\ndataset = CountingDataset(1000, max_length=8, vocab_size=3)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n\n# Transformer Model Parameters\nbatch_size = 32\nseq_len = 8\nd_model = 16\nnumber_of_heads = 2\nn = 2\ninput_vocab_size = 3  # Matches CountingDataset\noutput_vocab_size = 2  # Binary classification (0 or 1)\n\n# Instantiate Transformer Model\nmodel = Transformer(\n    batch_size=batch_size,\n    seq_len=seq_len,\n    d_model=d_model,\n    n=n,\n    number_of_heads=number_of_heads,\n    input_vocab_size=input_vocab_size,\n    output_vocab_size=output_vocab_size,\n)\n\n# Define Loss and Optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop\nepochs = 120\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for x, y in dataloader:\n        # Initialize decoder input with start-of-sequence tokens\n        decoder_x = torch.zeros_like(x)\n        decoder_x[:, 0] = 1  # Start-of-sequence token\n\n        # Forward pass\n        output = model(x, decoder_x)\n\n        # Use output at the last position for classification\n        logits = output[:, -1, :]  # Shape: (batch_size, output_vocab_size)\n        loss = criterion(logits, y)\n\n        total_loss += loss.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n\n# Testing the Model\nmodel.eval()\nwith torch.no_grad():\n    for test_data, test_labels in dataloader:\n        if test_data.size(0) != batch_size:\n            continue\n\n        decoder_x = torch.zeros_like(test_data)\n        decoder_x[:, 0] = 1  # Start-of-sequence token\n        output = model(test_data, decoder_x, apply_softmax=True)\n        predictions = torch.argmax(output[:, -1, :], dim=-1)\n\n        print(\"Test Labels:\", test_labels.numpy())\n        print(\"Predictions:\", predictions.numpy())\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:02:10.717839Z","iopub.execute_input":"2024-12-08T20:02:10.718336Z","iopub.status.idle":"2024-12-08T20:03:56.958195Z","shell.execute_reply.started":"2024-12-08T20:02:10.718293Z","shell.execute_reply":"2024-12-08T20:03:56.957242Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/120, Loss: 0.35190841621689256\nEpoch 2/120, Loss: 0.07408316766903285\nEpoch 3/120, Loss: 0.009030371964458496\nEpoch 4/120, Loss: 0.006707257185611994\nEpoch 5/120, Loss: 0.0055095166298410585\nEpoch 6/120, Loss: 0.004586713103156897\nEpoch 7/120, Loss: 0.0038638322899538662\nEpoch 8/120, Loss: 0.0032838567971221863\nEpoch 9/120, Loss: 0.002822262445284474\nEpoch 10/120, Loss: 0.002445638720546999\nEpoch 11/120, Loss: 0.0021384606258042396\nEpoch 12/120, Loss: 0.0018840808113436064\nEpoch 13/120, Loss: 0.0016705928513059212\nEpoch 14/120, Loss: 0.0014916687730639692\nEpoch 15/120, Loss: 0.0013388956994599393\nEpoch 16/120, Loss: 0.0012083472195832479\nEpoch 17/120, Loss: 0.001095338431637614\nEpoch 18/120, Loss: 0.0009979078876635721\nEpoch 19/120, Loss: 0.0009125375110025127\nEpoch 20/120, Loss: 0.0008377330358921279\nEpoch 21/120, Loss: 0.0007711813950370397\nEpoch 22/120, Loss: 0.0007119993209808824\nEpoch 23/120, Loss: 0.0006595107467634784\nEpoch 24/120, Loss: 0.000612302174446203\nEpoch 25/120, Loss: 0.0005701316479263046\nEpoch 26/120, Loss: 0.0005321505764919904\nEpoch 27/120, Loss: 0.0004975371177293241\nEpoch 28/120, Loss: 0.00046631390221356866\nEpoch 29/120, Loss: 0.0004377074421368419\nEpoch 30/120, Loss: 0.0004116247122686717\nEpoch 31/120, Loss: 0.00038779542721327276\nEpoch 32/120, Loss: 0.0003659178853981317\nEpoch 33/120, Loss: 0.0003457027872378427\nEpoch 34/120, Loss: 0.00032713883575202237\nEpoch 35/120, Loss: 0.0003098947016687523\nEpoch 36/120, Loss: 0.0002939964911239522\nEpoch 37/120, Loss: 0.0002792764571495354\nEpoch 38/120, Loss: 0.000265497132204473\nEpoch 39/120, Loss: 0.0002527778532596365\nEpoch 40/120, Loss: 0.00024083031683168826\nEpoch 41/120, Loss: 0.0002297376979127406\nEpoch 42/120, Loss: 0.0002192767893692719\nEpoch 43/120, Loss: 0.00020950024801066086\nEpoch 44/120, Loss: 0.00020042033875436192\nEpoch 45/120, Loss: 0.0001918077182527932\nEpoch 46/120, Loss: 0.00018371117626121567\nEpoch 47/120, Loss: 0.00017608820663524732\nEpoch 48/120, Loss: 0.0001689013203553435\nEpoch 49/120, Loss: 0.00016213491260479656\nEpoch 50/120, Loss: 0.0001557442921048571\nEpoch 51/120, Loss: 0.00014968429299657264\nEpoch 52/120, Loss: 0.00014392463931618558\nEpoch 53/120, Loss: 0.00013848143440085432\nEpoch 54/120, Loss: 0.00013333823552520404\nEpoch 55/120, Loss: 0.0001284405941166164\nEpoch 56/120, Loss: 0.00012379118185978563\nEpoch 57/120, Loss: 0.00011937532807707847\nEpoch 58/120, Loss: 0.00011514749794807886\nEpoch 59/120, Loss: 0.00011116044709436415\nEpoch 60/120, Loss: 0.00010733235033131354\nEpoch 61/120, Loss: 0.00010370490053901449\nEpoch 62/120, Loss: 0.00010020324266553226\nEpoch 63/120, Loss: 9.68881847051483e-05\nEpoch 64/120, Loss: 9.369963687101019e-05\nEpoch 65/120, Loss: 9.06682513642966e-05\nEpoch 66/120, Loss: 8.775918151161844e-05\nEpoch 67/120, Loss: 8.497783217594147e-05\nEpoch 68/120, Loss: 8.23130267822454e-05\nEpoch 69/120, Loss: 7.975492354166965e-05\nEpoch 70/120, Loss: 7.730423573276869e-05\nEpoch 71/120, Loss: 7.494750799753913e-05\nEpoch 72/120, Loss: 7.268822786676126e-05\nEpoch 73/120, Loss: 7.05212221345714e-05\nEpoch 74/120, Loss: 6.843412472423347e-05\nEpoch 75/120, Loss: 6.643377855652943e-05\nEpoch 76/120, Loss: 6.450023882338897e-05\nEpoch 77/120, Loss: 6.264948939544058e-05\nEpoch 78/120, Loss: 6.08554512455744e-05\nEpoch 79/120, Loss: 5.9135071839386174e-05\nEpoch 80/120, Loss: 5.747753394027842e-05\nEpoch 81/120, Loss: 5.5878636034791386e-05\nEpoch 82/120, Loss: 5.4337649940392905e-05\nEpoch 83/120, Loss: 5.28513400856736e-05\nEpoch 84/120, Loss: 5.141381306232013e-05\nEpoch 85/120, Loss: 5.003288090172704e-05\nEpoch 86/120, Loss: 4.8690762102318504e-05\nEpoch 87/120, Loss: 4.739610578267715e-05\nEpoch 88/120, Loss: 4.614783122838895e-05\nEpoch 89/120, Loss: 4.494065140005982e-05\nEpoch 90/120, Loss: 4.376928184008706e-05\nEpoch 91/120, Loss: 4.264020507464245e-05\nEpoch 92/120, Loss: 4.154645965524739e-05\nEpoch 93/120, Loss: 4.048936076991771e-05\nEpoch 94/120, Loss: 3.946458579570566e-05\nEpoch 95/120, Loss: 3.847069456242025e-05\nEpoch 96/120, Loss: 3.750876273824683e-05\nEpoch 97/120, Loss: 3.6569544458724255e-05\nEpoch 98/120, Loss: 3.567442599300956e-05\nEpoch 99/120, Loss: 3.479384712033695e-05\nEpoch 100/120, Loss: 3.39442694601002e-05\nEpoch 101/120, Loss: 3.312533273504326e-05\nEpoch 102/120, Loss: 3.232826800799117e-05\nEpoch 103/120, Loss: 3.1551028622479566e-05\nEpoch 104/120, Loss: 3.079950493561374e-05\nEpoch 105/120, Loss: 3.0068889827816747e-05\nEpoch 106/120, Loss: 2.9356658600473537e-05\nEpoch 107/120, Loss: 2.8672064922096568e-05\nEpoch 108/120, Loss: 2.799804954725738e-05\nEpoch 109/120, Loss: 2.734854452653728e-05\nEpoch 110/120, Loss: 2.671682270902825e-05\nEpoch 111/120, Loss: 2.6098443178794227e-05\nEpoch 112/120, Loss: 2.55025294585897e-05\nEpoch 113/120, Loss: 2.4920441231393675e-05\nEpoch 114/120, Loss: 2.4350962180065954e-05\nEpoch 115/120, Loss: 2.3804923447671406e-05\nEpoch 116/120, Loss: 2.3270534755635795e-05\nEpoch 117/120, Loss: 2.2746603375004093e-05\nEpoch 118/120, Loss: 2.224033772607424e-05\nEpoch 119/120, Loss: 2.174476128035871e-05\nEpoch 120/120, Loss: 2.1264812770119357e-05\nTest Labels: [0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0]\nPredictions: [0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T20:31:16.365302Z","iopub.execute_input":"2024-12-07T20:31:16.365700Z","iopub.status.idle":"2024-12-07T20:31:17.262836Z","shell.execute_reply.started":"2024-12-07T20:31:16.365666Z","shell.execute_reply":"2024-12-07T20:31:17.261805Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}