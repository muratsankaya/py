{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be00b49f7f254b3b9a7e1dc460693b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da757a28b8364738ae4cfb403428f615",
              "IPY_MODEL_2a889fd10f0b454180a4b64724a2e8c3",
              "IPY_MODEL_4b8fbdc5b4f94e25ac2af51e1c565e9b"
            ],
            "layout": "IPY_MODEL_8f77243da45148b7af7fcd2887732206"
          }
        },
        "da757a28b8364738ae4cfb403428f615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf36b88035a41258b5ec4cbc1bfd010",
            "placeholder": "​",
            "style": "IPY_MODEL_352b9a4d01a141728d5d3bd09558c833",
            "value": "Map: 100%"
          }
        },
        "2a889fd10f0b454180a4b64724a2e8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61ddf8c78e2a401db6f59453a62e4ea8",
            "max": 128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_774121cbaad24398bae81ecf3da102c4",
            "value": 128
          }
        },
        "4b8fbdc5b4f94e25ac2af51e1c565e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f7c9f2197fe450aabb8e3d7a3e7ec7b",
            "placeholder": "​",
            "style": "IPY_MODEL_710d2860306945178cb476b6890e7286",
            "value": " 128/128 [00:00&lt;00:00, 2171.58 examples/s]"
          }
        },
        "8f77243da45148b7af7fcd2887732206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf36b88035a41258b5ec4cbc1bfd010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "352b9a4d01a141728d5d3bd09558c833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61ddf8c78e2a401db6f59453a62e4ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774121cbaad24398bae81ecf3da102c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f7c9f2197fe450aabb8e3d7a3e7ec7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710d2860306945178cb476b6890e7286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vskzkRCDPKba",
        "outputId": "641c7c9d-3cb6-495e-c129-a6f20cbb056f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "!pip install datasets\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float32\n",
        "\n",
        "def softmax(x: torch.Tensor, d: int):\n",
        "    # return torch.nn.Softmax(dim=d)(x)\n",
        "    return torch.nn.functional.softmax(x, dim=d)\n",
        "\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 512,\n",
        "        hidden_size: list[tuple[int]] = None,\n",
        "        activation: torch.nn.Module = torch.nn.ReLU(),\n",
        "    ):\n",
        "        \"\"\"\n",
        "        notes:\n",
        "            - default dimensions are set according to paper\n",
        "\n",
        "        \"\"\"\n",
        "        if hidden_size is None:\n",
        "            hidden_size = [(d_model, 2048), (2048, d_model)]\n",
        "\n",
        "        assert len(hidden_size) > 0, \"hidden_size must be greater than 0\"\n",
        "        assert (\n",
        "            hidden_size[0][0] == d_model and hidden_size[-1][1] == d_model\n",
        "        ), \"input and output dimensions must equal d_model\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        n = len(hidden_size)\n",
        "        for i in range(n):\n",
        "            layers.append(torch.nn.Linear(*hidden_size[i]))\n",
        "\n",
        "            # No activation after the final layer\n",
        "            if i < n - 1:\n",
        "                layers.append(activation)\n",
        "\n",
        "        self.net = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def attention(\n",
        "    q: torch.Tensor,\n",
        "    k: torch.Tensor,\n",
        "    v: torch.Tensor,\n",
        "    pad_mask: torch.Tensor,\n",
        "    mask: torch.Tensor = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        q: a torch tensor of size: (batch_size*h, seq_len, d_k)\n",
        "        k: a torch tensor of size: (batch_size*h, seq_len, d_k)\n",
        "        v: a torch tensor of size: (batch_size*h, seq_len, d_v)\n",
        "        pad_mask: a torch tensor of size: (batch_size*h, seq_len, seq_len)\n",
        "        mask: a torch tensor of size: (seq_len, seq_len)\n",
        "\n",
        "    output:\n",
        "        A torch tensor of size: (batch_size*h, seq_len, d_v)\n",
        "\n",
        "    \"\"\"\n",
        "    x = torch.matmul(q, torch.transpose(k, 1, 2))\n",
        "\n",
        "    # Scale x by sqrt(d_k)\n",
        "    x = x / math.sqrt(q.size(2))\n",
        "\n",
        "    if mask is not None:\n",
        "\n",
        "        # Broadcasting will match the size at dim=0\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        # Apply mask via element-wise addition\n",
        "        x = x + mask\n",
        "\n",
        "    x = x + pad_mask\n",
        "\n",
        "    return torch.matmul(softmax(x, d=2), v)\n",
        "\n",
        "\n",
        "class MultiHead(torch.nn.Module):\n",
        "    def __init__(self, h: int = 8, d_model: int = 512, mask: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            h: number of heads\n",
        "            d_model: model dimensions, i.e. embedding size\n",
        "            mask: a boolean to apply masked multi-head attention\n",
        "        notes:\n",
        "            - default dimensions are set according to the paper\n",
        "\n",
        "        \"\"\"\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // h\n",
        "        self.d_v = self.d_k\n",
        "        self.h = h\n",
        "        self.mask = mask\n",
        "\n",
        "        self.w_q = torch.nn.Linear(d_model, d_model)\n",
        "        self.w_k = torch.nn.Linear(d_model, d_model)\n",
        "        self.w_v = torch.nn.Linear(d_model, d_model)\n",
        "        self.w_o = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(\n",
        "        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, pad_mask: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            q: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "            k: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "            v: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "            pad_mask: a torch tensor of size: (batch_size, seq_len, seq_len)\n",
        "\n",
        "        output:\n",
        "            A torch tensor of size: (batch_size, seq_len, d_model)\n",
        "\n",
        "        notes:\n",
        "            - d_model is essentially embedding dimensions\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = q.size()\n",
        "        q_h = self.w_q(q)\n",
        "        k_h = self.w_k(k)\n",
        "        v_h = self.w_v(v)\n",
        "\n",
        "        # Splitting q, k and v tensors in to h heads\n",
        "        q_h = q_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
        "        k_h = k_h.reshape(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
        "        v_h = v_h.reshape(batch_size, seq_len, self.h, self.d_v).transpose(1, 2)\n",
        "\n",
        "        # Combine heads for parrallel computation\n",
        "        q_h = q_h.reshape(batch_size * self.h, seq_len, self.d_k)\n",
        "        k_h = k_h.reshape(batch_size * self.h, seq_len, self.d_k)\n",
        "        v_h = v_h.reshape(batch_size * self.h, seq_len, self.d_v)\n",
        "\n",
        "        # Apply attention\n",
        "        scores = attention(q_h, k_h, v_h, pad_mask, mask=self.mask)\n",
        "\n",
        "        # Seperate heads\n",
        "        scores = scores.reshape(batch_size, self.h, seq_len, self.d_v).transpose(1, 2)\n",
        "\n",
        "        # Concat h heads (Concat(head1, ..., headh))\n",
        "        scores = scores.reshape(batch_size, seq_len, self.h * self.d_v)\n",
        "\n",
        "        return self.w_o(scores)\n",
        "\n",
        "\n",
        "# TODO: can change the definition so that if dropout called with\n",
        "# p=None then can just terminate. This would make the Encoder and\n",
        "# Decoder forward passes more concise\n",
        "def dropout(x: torch.Tensor, p: float = 0.1):\n",
        "    assert 0 <= p <= 1, \"p must be a probability\"\n",
        "\n",
        "    if p == 1:\n",
        "        # All elements are dropped; just return zeros.\n",
        "        return torch.zeros_like(x).to(device).to(dtype)\n",
        "\n",
        "    # Create a tensor with the same shape as x\n",
        "    # and set all is values to 1 - p\n",
        "    mask = torch.full_like(x, 1 - p).to(device).to(dtype)\n",
        "\n",
        "    # Will sample the entries from the bernoulli distribution.\n",
        "    # The i'th entry of the output tensor will draw a value 1 according\n",
        "    # to the i'th probability given the input tensor.\n",
        "    mask = torch.bernoulli(mask).to(device).to(dtype)\n",
        "\n",
        "    # Apply dropout via element-wise multiplication.\n",
        "    x = x * mask\n",
        "\n",
        "    # Apply inverted scaling\n",
        "    return x * (1 / (1 - p))\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, number_of_heads: int = 8, d_model: int = 512):\n",
        "        \"\"\"\n",
        "        notes:\n",
        "            - default parameter values are based on the paper\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            d_model % number_of_heads == 0\n",
        "        ), \"d_model must be divisible by number_of_heads\"\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n",
        "        self.feed_forward = FeedForward(d_model=d_model)\n",
        "        self.d_model = d_model\n",
        "        self.layer_norms = torch.nn.ModuleList(\n",
        "            torch.nn.LayerNorm(d_model) for _ in range(2)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, pad_mask: torch.Tensor, dropout_p=None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        input:\n",
        "            x: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "        output:\n",
        "            a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.layer_norms[0](\n",
        "            x\n",
        "            + (\n",
        "                self.multi_head_attention(x, x, x, pad_mask)\n",
        "                if dropout_p is None\n",
        "                else dropout(self.multi_head_attention(x, x, x, pad_mask), p=dropout_p)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return self.layer_norms[1](\n",
        "            x\n",
        "            + (\n",
        "                self.feed_forward(x)\n",
        "                if dropout_p is None\n",
        "                else dropout(self.feed_forward(x), p=dropout_p)\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_mask: torch.Tensor,\n",
        "        number_of_heads: int = 8,\n",
        "        d_model: int = 512,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        notes:\n",
        "            - default parameter values are based on the paper\n",
        "            - in the previous implementation we were using the same multi head\n",
        "            module and inidicating the apply mask on the forward pass. That's an\n",
        "            outregous mistake. The same parameters are being used in that case\n",
        "            only when applying attention part of the sequence was getting masked.\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            d_model % number_of_heads == 0\n",
        "        ), \"d_model must be divisible by number_of_heads\"\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHead(h=number_of_heads, d_model=d_model)\n",
        "        self.masked_multi_head_attention = MultiHead(\n",
        "            h=number_of_heads, d_model=d_model, mask=attention_mask\n",
        "        )\n",
        "        self.feed_forward = FeedForward(d_model=d_model)\n",
        "        self.d_model = d_model\n",
        "        self.layer_norms = torch.nn.ModuleList(\n",
        "            torch.nn.LayerNorm(d_model) for _ in range(3)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        encoder_x: torch.Tensor,\n",
        "        decoder_pad_mask: torch.Tensor,\n",
        "        encoder_pad_mask: torch.Tensor,\n",
        "        dropout_p=None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            x: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "            encoder_x: a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "        output:\n",
        "            a torch tensor of size: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = self.layer_norms[0](\n",
        "            x\n",
        "            + (\n",
        "                self.masked_multi_head_attention(x, x, x, decoder_pad_mask)\n",
        "                if dropout_p is None\n",
        "                else dropout(\n",
        "                    self.masked_multi_head_attention(x, x, x, decoder_pad_mask),\n",
        "                    p=dropout_p,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Cross-Attention\n",
        "        # Here encoder_pad_mask must be used because\n",
        "        x = self.layer_norms[1](\n",
        "            x\n",
        "            + (\n",
        "                self.multi_head_attention(x, encoder_x, encoder_x, encoder_pad_mask)\n",
        "                if dropout_p is None\n",
        "                else dropout(\n",
        "                    self.multi_head_attention(\n",
        "                        x, encoder_x, encoder_x, encoder_pad_mask\n",
        "                    ),\n",
        "                    p=dropout_p,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return self.layer_norms[2](\n",
        "            x\n",
        "            + (\n",
        "                self.feed_forward(x)\n",
        "                if dropout_p is None\n",
        "                else dropout(self.feed_forward(x), p=dropout_p)\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "def positional_encodings(seq_len: int, d_model: int) -> np.ndarray:\n",
        "    pos, i = np.indices((seq_len, d_model))\n",
        "    return np.where(\n",
        "        i % 2 == 0,\n",
        "        np.sin(pos / np.power(10000, (2 * i / d_model))),\n",
        "        np.cos(pos / np.power(10000, (2 * i / d_model))),\n",
        "    )\n",
        "\n",
        "\n",
        "def get_attention_mask(seq_len: int, dtype: torch.dtype) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Here we will be creating a triangular matrix where\n",
        "    all the upper triangle (above the diagonal) is set to -oo.\n",
        "\n",
        "    output:\n",
        "        A torch tensor of size: (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "\n",
        "    mask_x = torch.full((seq_len, seq_len), float(\"-inf\"), dtype=dtype).to(device).to(dtype)\n",
        "\n",
        "    return torch.triu(mask_x, diagonal=1).to(device).to(dtype)\n",
        "\n",
        "    # Add a new batch_size dimension and expand it to\n",
        "    # match batch_size\n",
        "    # -1 means keep the size at that dimension\n",
        "    # However its also documented here:\n",
        "    # https://stackoverflow.com/questions/65900110/does-pytorch-broadcast-consume-less-memory-than-expand\n",
        "    # that expand does not also consume extra memory\n",
        "    # mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "\n",
        "def get_pad_mask(\n",
        "    x: torch.Tensor, h: int, batch_size: int, seq_len: int, pad_token_id: int = 0\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "        x: a torch tensor of size: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    # 0 must be the padding token id\n",
        "    # Here torch.where(encoder_x == 0, float(\"-inf\"), 0)\n",
        "    #    .unsqueeze(2)\n",
        "    #    .expand(-1, -1, self.seq-len)\n",
        "    # should also mathematically produce the same shape\n",
        "    # but typically the keys gets masked so the current approach\n",
        "    # aligns better with the paper\n",
        "    assert x.size() == (\n",
        "        batch_size,\n",
        "        seq_len,\n",
        "    ), f\"x must have size: {(batch_size, seq_len)}\"\n",
        "\n",
        "    return (\n",
        "        torch.where(x == pad_token_id, float(\"-inf\"), 0)\n",
        "        .unsqueeze(1)\n",
        "        .unsqueeze(1)\n",
        "        .expand(-1, h, seq_len, -1)\n",
        "        .reshape(batch_size * h, seq_len, seq_len)\n",
        "        .to(device)\n",
        "        .to(dtype)\n",
        "    )\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "        seq_len: int,\n",
        "        d_model: int = 512,\n",
        "        n: int = 6,\n",
        "        number_of_heads: int = 8,\n",
        "        input_vocab_size: int = 37000,\n",
        "        output_vocab_size: int = 37000,\n",
        "        dtype: torch.dtype = torch.float32,\n",
        "        dropout_p: float = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            n: the number of encoder and decoder stacks\n",
        "            d_model: model dimensions, i.e. embedding dimensions\n",
        "            number_of_heads: number of heads using in multi-head attention\n",
        "            input_vocab_size: size of the vocabulary for the input structure\n",
        "            output_vocab_size: size of the vocabulary for the output structure\n",
        "\n",
        "        notes:\n",
        "            - I'm using a fixed seq_len for both the input and the output. That could\n",
        "            be adjusted to make it varied and more flexible.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.n = n\n",
        "        self.d_model = d_model\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.vocab_size = output_vocab_size\n",
        "        self.dtype = dtype\n",
        "        self.register_buffer(\n",
        "            \"pe\",\n",
        "            torch.from_numpy(positional_encodings(seq_len, d_model))\n",
        "            .to(dtype)\n",
        "            .unsqueeze(0)\n",
        "            .to(device)\n",
        "        )\n",
        "        self.register_buffer(\"attention_mask\", get_attention_mask(seq_len, dtype=dtype))\n",
        "        self.linear = torch.nn.Linear(d_model, output_vocab_size)\n",
        "        assert (\n",
        "            dropout_p is None or 0 <= dropout_p <= 1\n",
        "        ), \"p_dropout must be a value between 0 and 1\"\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.input_embedding = torch.nn.Embedding(\n",
        "            num_embeddings=input_vocab_size, embedding_dim=d_model\n",
        "        )\n",
        "        self.output_embedding = torch.nn.Embedding(\n",
        "            num_embeddings=output_vocab_size, embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "        # Using ModuleList is crucial here instead of python list\n",
        "        # If python list is used, then model.parameters() will not\n",
        "        # return the paremeters of layers inside.\n",
        "        self.encoders = torch.nn.ModuleList(\n",
        "            Encoder(number_of_heads=number_of_heads, d_model=d_model)\n",
        "            for _ in range(self.n)\n",
        "        )\n",
        "\n",
        "        self.decoders = torch.nn.ModuleList(\n",
        "            Decoder(\n",
        "                attention_mask=self.attention_mask,\n",
        "                number_of_heads=number_of_heads,\n",
        "                d_model=d_model,\n",
        "            )\n",
        "            for _ in range(self.n)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        encoder_x: torch.Tensor,\n",
        "        decoder_x: torch.Tensor,\n",
        "        apply_softmax: bool = False,\n",
        "        pad_token_id: int = 0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            encoder_x: a torch tensor of size: (batch_size, seq_len)\n",
        "            decoder_x: a torch tensor of size: (batch_size, seq_len)\n",
        "            apply_softmax: a boolean.\n",
        "                - Most torch loss functions expect logits instead of porbabilities.\n",
        "                So make sure that the loss function does not normalize inputs and\n",
        "                expects probabilities before setting this to True.\n",
        "\n",
        "        outputs:\n",
        "            a probability distribution over the vocabulary if apply_softmax is true else\n",
        "            it outputs logits\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            encoder_x.size() == decoder_x.size() == (self.batch_size, self.seq_len)\n",
        "        ), f\"encoder_x and decoder_x must both have the size: ({self.batch_size}, {self.seq_len})\"\n",
        "\n",
        "        assert torch.any(encoder_x) and torch.any(decoder_x), (\n",
        "            \"empty examples are not allowed. There could be some additional reasons \"\n",
        "            \"for not to allow them but simple example why is that softmax is not defined \"\n",
        "            \"over an empty sequence\"\n",
        "        )\n",
        "\n",
        "        encoder_x_pad_mask = get_pad_mask(\n",
        "            encoder_x,\n",
        "            h=self.number_of_heads,\n",
        "            batch_size=self.batch_size,\n",
        "            seq_len=self.seq_len,\n",
        "            pad_token_id=pad_token_id,\n",
        "        )\n",
        "        decoder_x_pad_mask = get_pad_mask(\n",
        "            decoder_x,\n",
        "            h=self.number_of_heads,\n",
        "            batch_size=self.batch_size,\n",
        "            seq_len=self.seq_len,\n",
        "            pad_token_id=pad_token_id,\n",
        "        )\n",
        "\n",
        "        # In the paper it is mentioned that they scale the embedding weights by math.sqrt(self.d_model)\n",
        "        # see the end of section 3.4 for more detail\n",
        "        encoder_x = self.input_embedding(encoder_x) * math.sqrt(self.d_model) + self.pe\n",
        "        decoder_x = self.output_embedding(decoder_x) * math.sqrt(self.d_model) + self.pe\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            encoder_x = encoder(encoder_x, encoder_x_pad_mask, dropout_p=self.dropout_p)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            decoder_x = decoder(\n",
        "                decoder_x,\n",
        "                encoder_x,\n",
        "                decoder_pad_mask=decoder_x_pad_mask,\n",
        "                encoder_pad_mask=encoder_x_pad_mask,\n",
        "                dropout_p=self.dropout_p,\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            softmax(self.linear(decoder_x), d=2)\n",
        "            if apply_softmax\n",
        "            else self.linear(decoder_x)\n",
        "        )"
      ],
      "metadata": {
        "id": "Q2C-oe1bPUrU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wmt14 = load_dataset(\"wmt14\", \"de-en\", split=\"train[:300000]\")"
      ],
      "metadata": {
        "id": "W1bK3nb3P0I8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "# Pre-trained English-German tokenizer/model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "\n",
        "# print(\"Tokenizer pad token:\", tokenizer.pad_token)  # prints: '<pad>'\n",
        "# print(\"Tokenizer pad token id:\", tokenizer.pad_token_id)  # prints: '58100'\n",
        "\n",
        "\n",
        "# Determine max length of tokenized sequences before padding\n",
        "# def length_check_function(examples):\n",
        "#     # Temporarily no truncation or padding\n",
        "#     src_lengths, tgt_lengths = [], []\n",
        "#     for translation in examples[\"translation\"]:\n",
        "#         src_lengths.append(len(tokenizer.tokenize(translation[\"de\"])))\n",
        "#         tgt_lengths.append(len(tokenizer.tokenize(translation[\"en\"])))\n",
        "\n",
        "#     return {\"src_length\": src_lengths, \"tgt_length\": tgt_lengths}\n",
        "\n",
        "\n",
        "# lengths = wmt14[\"train\"].map(length_check_function, batched=True)\n",
        "# max_source_length = max(lengths[\"src_length\"])\n",
        "# max_target_length = max(lengths[\"tgt_length\"])\n",
        "# max_seq_len = max(max_source_length, max_target_length)\n",
        "# print(\"Max sequence length:\", max_seq_len)  # prints: '13,614'\n",
        "\n",
        "max_seq_len = 32\n",
        "\n",
        "\n",
        "if tokenizer.bos_token is None:\n",
        "    tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
        "    # If a model is already loaded, you may need:\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize source (German)\n",
        "    model_inputs = tokenizer(\n",
        "        [translation[\"de\"] for translation in examples[\"translation\"]],\n",
        "        max_length=max_seq_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # Tokenize target (English)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        tokenized_targets = tokenizer(\n",
        "            [translation[\"en\"] for translation in examples[\"translation\"]],\n",
        "            max_length=max_seq_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "    input_ids = tokenized_targets[\"input_ids\"]\n",
        "\n",
        "    # Ensure bos_token_id is defined (after adding bos_token if needed)\n",
        "    bos_id = tokenizer.bos_token_id\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Create decoder_input_ids by prepending BOS and removing the last token\n",
        "    # Example:\n",
        "    # original input_ids: [w1, w2, w3, eos]\n",
        "    # decoder_input_ids:  [bos_id, w1, w2, w3]\n",
        "    # labels:             [w1, w2, w3, eos]\n",
        "    decoder_input_ids = [\n",
        "        [bos_id] + [seq_id for seq_id in seq if seq_id != eos_id] for seq in input_ids\n",
        "    ]\n",
        "\n",
        "    # When indicies are set to -100 they are ignored in loss computation of cross entropy\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    labels = [\n",
        "        [(seq_id if seq_id != pad_id else -100) for seq_id in seq] for seq in input_ids\n",
        "    ]\n",
        "\n",
        "    # Update model_inputs\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
        "    model_inputs[\"decoder_attention_mask\"] = tokenized_targets[\"attention_mask\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Select a subset of the training set\n",
        "wmt14_subset = wmt14.select(range(128))\n",
        "\n",
        "# Now apply your preprocessing function to just those 128 examples\n",
        "tokenized_wmt14_subset = wmt14_subset.map(\n",
        "    preprocess_function, batched=True, remove_columns=wmt14.column_names\n",
        ")\n",
        "\n",
        "print(\"tokenized_wmt14_subset\", tokenized_wmt14_subset)\n",
        "\n",
        "# Convert to torch for consistency\n",
        "tokenized_wmt14_subset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\n",
        "        \"input_ids\",\n",
        "        \"decoder_input_ids\",\n",
        "        \"labels\",\n",
        "        \"attention_mask\",\n",
        "        \"decoder_attention_mask\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Checking if the tokenized_wmt14 dataset is cached correctly: {tokenized_wmt14_subset.cache_files}\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Attention mask size\", tokenized_wmt14_subset[\"attention_mask\"][0].size())\n",
        "print(\n",
        "    \"Decoder Attention mask size\",\n",
        "    tokenized_wmt14_subset[\"decoder_attention_mask\"].size(),\n",
        ")\n",
        "\n",
        "# Now you can print out some examples to visually inspect them\n",
        "for i in range(3):\n",
        "    print(\"Example\", i)\n",
        "    print(\"Input IDs:\", tokenized_wmt14_subset[i][\"input_ids\"])\n",
        "    print(\"Decoder Input IDs:\", tokenized_wmt14_subset[i][\"decoder_input_ids\"])\n",
        "    print(\"Labels:\", tokenized_wmt14_subset[i][\"labels\"])\n",
        "    print(\"Encoder Attention Mask:\", tokenized_wmt14_subset[i][\"attention_mask\"])\n",
        "    print(\n",
        "        \"Decoder Attention Mask:\", tokenized_wmt14_subset[i][\"decoder_attention_mask\"]\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"decoded_input size: {tokenized_wmt14_subset[i]['input_ids'].size()}\",\n",
        "        \"Decoded input:\",\n",
        "        tokenizer.decode(tokenized_wmt14_subset[i][\"input_ids\"]).replace(\" <pad>\", \"\"),\n",
        "    )\n",
        "    print(\n",
        "        f\"decoded_decoder_input size: {tokenized_wmt14_subset[i]['decoder_input_ids'].size()}\",\n",
        "        \"Decoded decoder input:\",\n",
        "        tokenizer.decode(tokenized_wmt14_subset[i][\"decoder_input_ids\"])\n",
        "        .replace(\" <pad>\", \"\")\n",
        "        .replace(\"<pad>\", \"\"),\n",
        "    )\n",
        "    # print(\n",
        "    #     f\"decoded_labels size: {tokenized_wmt14_subset[i]['labels'].size()}\",\n",
        "    #     \"Decoded labels:\",\n",
        "    #     tokenizer.decode(tokenized_wmt14_subset[i][\"labels\"]).replace(\" <pad>\", \"\"),\n",
        "    # )\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "be00b49f7f254b3b9a7e1dc460693b4a",
            "da757a28b8364738ae4cfb403428f615",
            "2a889fd10f0b454180a4b64724a2e8c3",
            "4b8fbdc5b4f94e25ac2af51e1c565e9b",
            "8f77243da45148b7af7fcd2887732206",
            "bdf36b88035a41258b5ec4cbc1bfd010",
            "352b9a4d01a141728d5d3bd09558c833",
            "61ddf8c78e2a401db6f59453a62e4ea8",
            "774121cbaad24398bae81ecf3da102c4",
            "5f7c9f2197fe450aabb8e3d7a3e7ec7b",
            "710d2860306945178cb476b6890e7286"
          ]
        },
        "id": "UMqapONIP28R",
        "outputId": "b1ddf987-5d6b-4992-860f-b534f12df5c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be00b49f7f254b3b9a7e1dc460693b4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_wmt14_subset Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids', 'decoder_attention_mask'],\n",
            "    num_rows: 128\n",
            "})\n",
            "Checking if the tokenized_wmt14 dataset is cached correctly: [{'filename': '/root/.cache/huggingface/datasets/wmt14/de-en/0.0.0/b199e406369ec1b7634206d3ded5ba45de2fe696/cache-1c4a3504b38f2f53.arrow'}]\n",
            "Attention mask size torch.Size([32])\n",
            "Decoder Attention mask size torch.Size([128, 32])\n",
            "Example 0\n",
            "Input IDs: tensor([35999,     9,  4371, 25478,     0, 58100, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100])\n",
            "Decoder Input IDs: tensor([58101,   465, 32932,     7,     4,  6274, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100,\n",
            "        58100, 58100])\n",
            "Labels: tensor([  465, 32932,     7,     4,  6274,     0,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100])\n",
            "Encoder Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Decoder Attention Mask: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "decoded_input size: torch.Size([32]) Decoded input: Wiederaufnahme der Sitzungsperiode</s>\n",
            "decoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> Resumption of the session\n",
            "\n",
            "Example 1\n",
            "Input IDs: tensor([  105, 34827,    11,   121,  5736,     2,    57,  4675,  1128, 25548,\n",
            "           18,  4371, 25478,    40,   320,  1396,    28,   427,  1726,  9692,\n",
            "            2, 23987,   373, 17617,   667,  8448,   123,  1926,  5806,    10,\n",
            "         6497,     0])\n",
            "Decoder Input IDs: tensor([58101,    38, 18950, 38309,     4,  6274,     7,     4,   151,   626,\n",
            "         3059, 29423,  8575,    32,  5682,   507,  1088, 17129,     8,    38,\n",
            "          206,   209,  1430,   696,    12,  2455,    41,    14,  3360,   168,\n",
            "          332,     5])\n",
            "Labels: tensor([   38, 18950, 38309,     4,  6274,     7,     4,   151,   626,  3059,\n",
            "        29423,  8575,    32,  5682,   507,  1088, 17129,     8,    38,   206,\n",
            "          209,  1430,   696,    12,  2455,    41,    14,  3360,   168,   332,\n",
            "            5,     0])\n",
            "Encoder Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Decoder Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "decoded_input size: torch.Size([32]) Decoded input: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe</s>\n",
            "decoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in\n",
            "\n",
            "Example 2\n",
            "Input IDs: tensor([  423,    42, 11967,  2985,     2,    29,     9,   372,  4209,  8319,\n",
            "          227,    47,   387, 26050,  1540,    13,   188,  2597,    47,    51,\n",
            "        28970,     3,  2436,    70,  2435,  9449,   592,   445,  4655,    21,\n",
            "        13932,     0])\n",
            "Decoder Input IDs: tensor([58101,  4677,     2,    52,    41,    73,    72,  1770,     2,     4,\n",
            "        48186,   108,   255, 24526,    15, 44540, 19113,    22,  9326,    12,\n",
            "         1337,  2029,     2,   530,     4,   238,     5,    14,   438,     7,\n",
            "          419, 12971])\n",
            "Labels: tensor([ 4677,     2,    52,    41,    73,    72,  1770,     2,     4, 48186,\n",
            "          108,   255, 24526,    15, 44540, 19113,    22,  9326,    12,  1337,\n",
            "         2029,     2,   530,     4,   238,     5,    14,   438,     7,   419,\n",
            "        12971,     0])\n",
            "Encoder Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Decoder Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "decoded_input size: torch.Size([32]) Decoded input: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklich</s>\n",
            "decoded_decoder_input size: torch.Size([32]) Decoded decoder input: <s> Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save(filename, **kwargs):\n",
        "    \"\"\"\n",
        "    Save a pytorch object to file\n",
        "    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "    Arguments:\n",
        "        filename: the file in which to save the object\n",
        "\n",
        "    Possible keyword arguments (kwargs):\n",
        "        epoch: the epoch so far if training\n",
        "        model_state_dict: a model's state\n",
        "        opt_state_dict: a optimizer's state, if training\n",
        "    \"\"\"\n",
        "\n",
        "    msg = f\"{filename} exists: delete it first to replace it.\"\n",
        "    assert not os.path.exists(filename), msg\n",
        "    torch.save(kwargs, filename)\n",
        "\n",
        "\n",
        "def load(filename):\n",
        "    \"\"\"\n",
        "    Load a pytorch object from a given filename\n",
        "    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    You shouldn't need to edit this function.\n",
        "\n",
        "    Arguments:\n",
        "        filename: the file from which to load the object\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.load(filename)\n"
      ],
      "metadata": {
        "id": "uKL8hPNCQldJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, optimizer, model, loss_func, scheduler=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the optimizer for the model, using any necessary kwargs\n",
        "        Save the model and loss function for later calculation\n",
        "        You shouldn't need to edit this function.\n",
        "        \"\"\"\n",
        "\n",
        "        self.optimizer: torch.optim.Optimizer = optimizer(model.parameters(), **kwargs)\n",
        "        self.scheduler = scheduler[\"scheduler\"](\n",
        "            self.optimizer, lr_lambda=scheduler[\"lr_lambda\"]\n",
        "        )\n",
        "        self.model: torch.nn.Module = model\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "        self.epoch = 0\n",
        "        self.start_time = None\n",
        "\n",
        "    def run_one_batch(self, encder_x, decoder_x, y, train=True, pad_token_id=0):\n",
        "        \"\"\"\n",
        "        Run self.model on one batch of data, using `self.loss_func` to\n",
        "            compute the model's loss.\n",
        "\n",
        "        If train=True (the default), you should use `self.optimizer`\n",
        "            to update the parameters of `self.model`.\n",
        "\n",
        "        You should also call `self.optimizer.zero_grad()`; see\n",
        "            https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
        "            for a guide as to when to do that.\n",
        "\n",
        "        Args\n",
        "            enocder_x: the batch's enocder input\n",
        "            decoder_x: the batch's decoder input\n",
        "            y: the batch's target\n",
        "\n",
        "        Returns\n",
        "            loss: the model's loss on this batch\n",
        "        \"\"\"\n",
        "\n",
        "        if train:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        outputs = self.model(encder_x.to(device),\n",
        "                             decoder_x.to(device),\n",
        "                             pad_token_id=pad_token_id)\n",
        "        loss = self.loss_func(outputs.transpose(1, 2),\n",
        "                              y.to(device))\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "\n",
        "        return loss.detach().cpu().numpy()\n",
        "\n",
        "    def run_one_epoch(\n",
        "        self,\n",
        "        data_loader: torch.utils.data.DataLoader,\n",
        "        train=True,\n",
        "        verbose=False,\n",
        "        pad_token_id=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train one epoch, a batch at a time, using self.run_one_batch\n",
        "        You shouldn't need to edit this function.\n",
        "\n",
        "        Args:\n",
        "            data_loader: a torch.utils.data.DataLoader with our dataset\n",
        "            stats: an optional dict of information to print out\n",
        "\n",
        "        Returns:\n",
        "            total_loss: the average loss per example\n",
        "        \"\"\"\n",
        "        np.random.seed(0)\n",
        "        torch.manual_seed(0)\n",
        "        # torch.use_deterministic_algorithms(True)\n",
        "        if self.start_time is None:\n",
        "            self.start_time = time.time()\n",
        "\n",
        "        # The following adjustment in the loss computation\n",
        "        # was made, becase torch.nn.CrossEntropyLoss returns\n",
        "        # the avg. loss for each batch\n",
        "        # epoch_size = 0\n",
        "        batch_count = 0\n",
        "        batch_size = 0\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            encoder_x, decoder_x, y = (\n",
        "                batch_data[\"input_ids\"],\n",
        "                batch_data[\"decoder_input_ids\"],\n",
        "                batch_data[\"labels\"],\n",
        "            )\n",
        "            if batch_idx == 0:\n",
        "              batch_size = encoder_x.size(0)\n",
        "            # epoch_size += encoder_x.size(0)\n",
        "            batch_count += encoder_x.size(0) / batch_size\n",
        "            loss = self.run_one_batch(\n",
        "                encoder_x, decoder_x, y, train=train, pad_token_id=pad_token_id\n",
        "            )\n",
        "            total_loss += loss\n",
        "\n",
        "        avg_loss = total_loss / batch_count\n",
        "\n",
        "        if verbose:\n",
        "            epoch = self.epoch + 1\n",
        "            duration = (time.time() - self.start_time) / 60\n",
        "\n",
        "            if train:\n",
        "                log = [f\"Epoch: {epoch:6d}\"]\n",
        "            else:\n",
        "                log = [\"Eval:\" + \" \" * 8]\n",
        "\n",
        "            log.extend(\n",
        "                [\n",
        "                    f\"Loss: {avg_loss:6.3f}\",\n",
        "                    f\"in {duration:5.1f} min\",\n",
        "                ]\n",
        "            )\n",
        "            print(\"  \".join(log))\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def train(\n",
        "        self, data_loader, n_epochs, train=True, report_every=None, pad_token_id=0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the model for `n_epochs` epochs on the data in `data_loader`\n",
        "        You shouldn't need to edit this function.\n",
        "\n",
        "        Args\n",
        "            data_loader: data loader for our data\n",
        "            n_epochs: how many epochs to run\n",
        "            train: if True, train the model; otherwise, just evaluate it\n",
        "            report_every: how often to print out stats\n",
        "\n",
        "        Returns\n",
        "            losses: average loss per epoch\n",
        "        \"\"\"\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        if report_every is None:\n",
        "            report_every = max(1, n_epochs // 10)\n",
        "\n",
        "        losses = []\n",
        "        for i in range(n_epochs):\n",
        "            verbose = ((i + 1) % report_every) == 0\n",
        "            loss = self.run_one_epoch(\n",
        "                data_loader, train=train, verbose=verbose, pad_token_id=pad_token_id\n",
        "            )\n",
        "            losses.append(loss)\n",
        "            if train:\n",
        "                self.epoch += 1\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def eval(self, data_loader):\n",
        "        \"\"\"\n",
        "        Helper function to run through the data loader once and just\n",
        "            compute the loss\n",
        "        You shouldn't need to edit this function.\n",
        "        \"\"\"\n",
        "        return self.train(data_loader, 1, train=False, report_every=1)\n",
        "\n",
        "    def save_trainer(self, filename):\n",
        "        \"\"\"\n",
        "        Use `src.utils.save` to save this Trainer to file.\n",
        "        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "        Args\n",
        "            filename: the file to which to save the trainer\n",
        "        \"\"\"\n",
        "        save(\n",
        "            filename,\n",
        "            epoch=self.epoch,\n",
        "            model_state_dict=self.model.state_dict(),\n",
        "            optimizer_state_dict=self.optimizer.state_dict(),\n",
        "        )\n",
        "\n",
        "    def load_trainer(self, filename):\n",
        "        \"\"\"\n",
        "        Use `src.utils.load` to load this trainer from file.\n",
        "        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "        Note: in addition to simply loading the saved model, you must\n",
        "            use the information from that checkpoint to update the model's\n",
        "            state.\n",
        "\n",
        "        Args\n",
        "            filename: the file from which to load the model\n",
        "        \"\"\"\n",
        "        checkpoint = load(filename)\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        self.epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "        # this ensures that the trainer's/model's parameters are in training mode\n",
        "        self.model.train()\n"
      ],
      "metadata": {
        "id": "n9w5JrCiQwAR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "data_loader = DataLoader(tokenized_wmt14_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Transformer(\n",
        "    batch_size=batch_size,\n",
        "    seq_len=32,\n",
        "    d_model=64,\n",
        "    # +1 for bos token\n",
        "    input_vocab_size=tokenizer.vocab_size + 1,\n",
        "    output_vocab_size=tokenizer.vocab_size + 1,\n",
        "    dropout_p=0.1,\n",
        ").to(device)\n",
        "\n",
        "d_model = 64\n",
        "\n",
        "\n",
        "def lr_lambda(step_num):\n",
        "    if step_num == 0:\n",
        "        step_num = 1\n",
        "    return d_model ** (-0.5) * step_num ** (-0.5)\n",
        "\n",
        "\n",
        "trainer_args = {\n",
        "    \"lr\": 1.0,\n",
        "    \"optimizer\": Adam,\n",
        "}\n",
        "\n",
        "scheulder = {\"scheduler\": lr_scheduler.LambdaLR, \"lr_lambda\": lr_lambda}\n",
        "trainer = Trainer(\n",
        "    model=model, loss_func=CrossEntropyLoss(), scheduler=scheulder, **trainer_args\n",
        ")\n",
        "\n",
        "trainer.train(data_loader=data_loader, n_epochs=500, pad_token_id=tokenizer.pad_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVxD8wT4RWhZ",
        "outputId": "eed24014-9992-43c2-9f15-4e2aef3383e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:     50  Loss:  3.053  in   0.1 min\n",
            "Epoch:    100  Loss:  2.112  in   0.2 min\n",
            "Epoch:    150  Loss:  1.841  in   0.3 min\n",
            "Epoch:    200  Loss:  1.142  in   0.4 min\n",
            "Epoch:    250  Loss:  0.038  in   0.5 min\n",
            "Epoch:    300  Loss:  0.007  in   0.5 min\n",
            "Epoch:    350  Loss:  0.004  in   0.6 min\n",
            "Epoch:    400  Loss:  0.003  in   0.7 min\n",
            "Epoch:    450  Loss:  0.002  in   0.8 min\n",
            "Epoch:    500  Loss:  0.001  in   0.9 min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.559109210968018,\n",
              " 6.98730206489563,\n",
              " 6.339357852935791,\n",
              " 5.875925064086914,\n",
              " 5.787480592727661,\n",
              " 5.585981607437134,\n",
              " 5.426644325256348,\n",
              " 5.268503904342651,\n",
              " 5.181648254394531,\n",
              " 5.1363959312438965,\n",
              " 5.075185537338257,\n",
              " 5.000913143157959,\n",
              " 4.9370667934417725,\n",
              " 4.879709005355835,\n",
              " 4.818930387496948,\n",
              " 4.757419109344482,\n",
              " 4.699483156204224,\n",
              " 4.644929647445679,\n",
              " 4.590926885604858,\n",
              " 4.535322189331055,\n",
              " 4.478294372558594,\n",
              " 4.421422243118286,\n",
              " 4.365188837051392,\n",
              " 4.30874490737915,\n",
              " 4.251906394958496,\n",
              " 4.1955389976501465,\n",
              " 4.140211582183838,\n",
              " 4.0855807065963745,\n",
              " 4.0311479568481445,\n",
              " 3.976737856864929,\n",
              " 3.9225406646728516,\n",
              " 3.869032144546509,\n",
              " 3.8164122104644775,\n",
              " 3.76452100276947,\n",
              " 3.7133803367614746,\n",
              " 3.663103461265564,\n",
              " 3.613702416419983,\n",
              " 3.5652259588241577,\n",
              " 3.5176515579223633,\n",
              " 3.4708563089370728,\n",
              " 3.424820065498352,\n",
              " 3.3796472549438477,\n",
              " 3.3354674577713013,\n",
              " 3.292313814163208,\n",
              " 3.2501442432403564,\n",
              " 3.2088959217071533,\n",
              " 3.168515205383301,\n",
              " 3.129046320915222,\n",
              " 3.0905673503875732,\n",
              " 3.0531065464019775,\n",
              " 3.016686201095581,\n",
              " 2.981305241584778,\n",
              " 2.946947693824768,\n",
              " 2.91359543800354,\n",
              " 2.8812334537506104,\n",
              " 2.8498517274856567,\n",
              " 2.819445848464966,\n",
              " 2.7900010347366333,\n",
              " 2.761492133140564,\n",
              " 2.7339009046554565,\n",
              " 2.7072155475616455,\n",
              " 2.681416869163513,\n",
              " 2.6564695835113525,\n",
              " 2.6323312520980835,\n",
              " 2.6089731454849243,\n",
              " 2.586378335952759,\n",
              " 2.5645233392715454,\n",
              " 2.5433762073516846,\n",
              " 2.5229082107543945,\n",
              " 2.503090500831604,\n",
              " 2.483897566795349,\n",
              " 2.4653027057647705,\n",
              " 2.4472824335098267,\n",
              " 2.4298126697540283,\n",
              " 2.41286563873291,\n",
              " 2.3964136838912964,\n",
              " 2.3804376125335693,\n",
              " 2.364922881126404,\n",
              " 2.34985888004303,\n",
              " 2.3352195024490356,\n",
              " 2.3209861516952515,\n",
              " 2.307141900062561,\n",
              " 2.2936787605285645,\n",
              " 2.2805784940719604,\n",
              " 2.267826557159424,\n",
              " 2.2554101943969727,\n",
              " 2.2433226108551025,\n",
              " 2.2315536737442017,\n",
              " 2.220090389251709,\n",
              " 2.208922266960144,\n",
              " 2.198040246963501,\n",
              " 2.1874366998672485,\n",
              " 2.177102208137512,\n",
              " 2.1670281887054443,\n",
              " 2.157209873199463,\n",
              " 2.1476410031318665,\n",
              " 2.138312339782715,\n",
              " 2.129218101501465,\n",
              " 2.120351195335388,\n",
              " 2.111704468727112,\n",
              " 2.1032694578170776,\n",
              " 2.095040738582611,\n",
              " 2.087014079093933,\n",
              " 2.079183042049408,\n",
              " 2.071541428565979,\n",
              " 2.0640829205513,\n",
              " 2.056799292564392,\n",
              " 2.0496855974197388,\n",
              " 2.042733371257782,\n",
              " 2.035938262939453,\n",
              " 2.029298424720764,\n",
              " 2.0228074193000793,\n",
              " 2.016457736492157,\n",
              " 2.0102427005767822,\n",
              " 2.004158616065979,\n",
              " 1.998200237751007,\n",
              " 1.9923644661903381,\n",
              " 1.9866434931755066,\n",
              " 1.981033980846405,\n",
              " 1.975529968738556,\n",
              " 1.9701270461082458,\n",
              " 1.964821457862854,\n",
              " 1.9596103429794312,\n",
              " 1.9544927477836609,\n",
              " 1.949458122253418,\n",
              " 1.9445064067840576,\n",
              " 1.9396334886550903,\n",
              " 1.934838354587555,\n",
              " 1.930112898349762,\n",
              " 1.9254530668258667,\n",
              " 1.9208531975746155,\n",
              " 1.916315495967865,\n",
              " 1.911837875843048,\n",
              " 1.9074174761772156,\n",
              " 1.903050422668457,\n",
              " 1.8987329602241516,\n",
              " 1.894460141658783,\n",
              " 1.8902289271354675,\n",
              " 1.8860417008399963,\n",
              " 1.8818880915641785,\n",
              " 1.877760887145996,\n",
              " 1.8736577033996582,\n",
              " 1.8695783615112305,\n",
              " 1.8655217289924622,\n",
              " 1.861482858657837,\n",
              " 1.8574575781822205,\n",
              " 1.8534349203109741,\n",
              " 1.8494105339050293,\n",
              " 1.8453830480575562,\n",
              " 1.841341495513916,\n",
              " 1.8372774124145508,\n",
              " 1.8331890106201172,\n",
              " 1.8290663957595825,\n",
              " 1.8248911499977112,\n",
              " 1.8206533789634705,\n",
              " 1.816364347934723,\n",
              " 1.812002718448639,\n",
              " 1.8075281977653503,\n",
              " 1.8029227256774902,\n",
              " 1.79818993806839,\n",
              " 1.7933006286621094,\n",
              " 1.7882407307624817,\n",
              " 1.7829503417015076,\n",
              " 1.7774139046669006,\n",
              " 1.7715489268302917,\n",
              " 1.7653403878211975,\n",
              " 1.758830726146698,\n",
              " 1.7517861127853394,\n",
              " 1.744200587272644,\n",
              " 1.7360403537750244,\n",
              " 1.7271788120269775,\n",
              " 1.7173996567726135,\n",
              " 1.7068125009536743,\n",
              " 1.6949321627616882,\n",
              " 1.6820083260536194,\n",
              " 1.6680343747138977,\n",
              " 1.6534267663955688,\n",
              " 1.6374244689941406,\n",
              " 1.6202671527862549,\n",
              " 1.6016568541526794,\n",
              " 1.5904588103294373,\n",
              " 1.5818017721176147,\n",
              " 1.5708845257759094,\n",
              " 1.5414166450500488,\n",
              " 1.518242061138153,\n",
              " 1.494308590888977,\n",
              " 1.474194347858429,\n",
              " 1.450135588645935,\n",
              " 1.425679326057434,\n",
              " 1.4017947316169739,\n",
              " 1.3712769150733948,\n",
              " 1.3488059043884277,\n",
              " 1.316752314567566,\n",
              " 1.2896218299865723,\n",
              " 1.2613471150398254,\n",
              " 1.2402504086494446,\n",
              " 1.2183858156204224,\n",
              " 1.2024842500686646,\n",
              " 1.1826062202453613,\n",
              " 1.1416635811328888,\n",
              " 1.092625230550766,\n",
              " 1.062482476234436,\n",
              " 1.02261021733284,\n",
              " 0.982264906167984,\n",
              " 0.9575645923614502,\n",
              " 0.9172284007072449,\n",
              " 0.8772529661655426,\n",
              " 0.8430892527103424,\n",
              " 0.7947779595851898,\n",
              " 0.7581321895122528,\n",
              " 0.7315734326839447,\n",
              " 0.6883203387260437,\n",
              " 0.6508739590644836,\n",
              " 0.6284085214138031,\n",
              " 0.5749973207712173,\n",
              " 0.5593769252300262,\n",
              " 0.5173050612211227,\n",
              " 0.5111507922410965,\n",
              " 0.5051448196172714,\n",
              " 0.48494891822338104,\n",
              " 0.4318576008081436,\n",
              " 0.39749211072921753,\n",
              " 0.36865270137786865,\n",
              " 0.3452085703611374,\n",
              " 0.3287424147129059,\n",
              " 0.3030553013086319,\n",
              " 0.2729710713028908,\n",
              " 0.2555662766098976,\n",
              " 0.2308061346411705,\n",
              " 0.21272994577884674,\n",
              " 0.19355931133031845,\n",
              " 0.1756371632218361,\n",
              " 0.1584228053689003,\n",
              " 0.14546668156981468,\n",
              " 0.1333385333418846,\n",
              " 0.12195800244808197,\n",
              " 0.10948335751891136,\n",
              " 0.10117772594094276,\n",
              " 0.09165116399526596,\n",
              " 0.08281970024108887,\n",
              " 0.0766095407307148,\n",
              " 0.07007390074431896,\n",
              " 0.06441328302025795,\n",
              " 0.05926208756864071,\n",
              " 0.05465375632047653,\n",
              " 0.050484685227274895,\n",
              " 0.046798236668109894,\n",
              " 0.043409042060375214,\n",
              " 0.04054469056427479,\n",
              " 0.03781966492533684,\n",
              " 0.03534144163131714,\n",
              " 0.03327996842563152,\n",
              " 0.03130547981709242,\n",
              " 0.029489915817975998,\n",
              " 0.027862289920449257,\n",
              " 0.02641707006841898,\n",
              " 0.025045236572623253,\n",
              " 0.02381996624171734,\n",
              " 0.022708569653332233,\n",
              " 0.02167756762355566,\n",
              " 0.020722731947898865,\n",
              " 0.01984468847513199,\n",
              " 0.019047332927584648,\n",
              " 0.018301324918866158,\n",
              " 0.017607005313038826,\n",
              " 0.016954777762293816,\n",
              " 0.016352254897356033,\n",
              " 0.015791798941791058,\n",
              " 0.015264856163412333,\n",
              " 0.014773624949157238,\n",
              " 0.01430809823796153,\n",
              " 0.013870735187083483,\n",
              " 0.013461742550134659,\n",
              " 0.013075907714664936,\n",
              " 0.012706778477877378,\n",
              " 0.012356297578662634,\n",
              " 0.012027163989841938,\n",
              " 0.011714918538928032,\n",
              " 0.011416201945394278,\n",
              " 0.011130208149552345,\n",
              " 0.010857727844268084,\n",
              " 0.010599635541439056,\n",
              " 0.010353788267821074,\n",
              " 0.010116184130311012,\n",
              " 0.009889013133943081,\n",
              " 0.009671628009527922,\n",
              " 0.009464781265705824,\n",
              " 0.009263598825782537,\n",
              " 0.009070496540516615,\n",
              " 0.008886230178177357,\n",
              " 0.008708388544619083,\n",
              " 0.008536623325198889,\n",
              " 0.008370775263756514,\n",
              " 0.008210769854485989,\n",
              " 0.008058315608650446,\n",
              " 0.007909514708444476,\n",
              " 0.007765117799863219,\n",
              " 0.007625208236277103,\n",
              " 0.007491734577342868,\n",
              " 0.007362228352576494,\n",
              " 0.007235468598082662,\n",
              " 0.0071133754681795835,\n",
              " 0.006995619973167777,\n",
              " 0.006881184410303831,\n",
              " 0.0067706189583987,\n",
              " 0.006662530126050115,\n",
              " 0.006557089276611805,\n",
              " 0.006455361610278487,\n",
              " 0.006357002537697554,\n",
              " 0.006260166643187404,\n",
              " 0.006166630424559116,\n",
              " 0.006076257675886154,\n",
              " 0.0059877855237573385,\n",
              " 0.005901435622945428,\n",
              " 0.005817883415147662,\n",
              " 0.005736386403441429,\n",
              " 0.005656770896166563,\n",
              " 0.005578808952122927,\n",
              " 0.005503035383298993,\n",
              " 0.0054289656691253185,\n",
              " 0.005356830777600408,\n",
              " 0.005286548752337694,\n",
              " 0.005217638798058033,\n",
              " 0.005150699522346258,\n",
              " 0.005084737204015255,\n",
              " 0.005021314835175872,\n",
              " 0.004958490375429392,\n",
              " 0.004897385369986296,\n",
              " 0.004837134387344122,\n",
              " 0.0047785439528524876,\n",
              " 0.004721435019746423,\n",
              " 0.004665436688810587,\n",
              " 0.004610552452504635,\n",
              " 0.004557032370939851,\n",
              " 0.004504194483160973,\n",
              " 0.004452874651178718,\n",
              " 0.004402641439810395,\n",
              " 0.004353316500782967,\n",
              " 0.004304976435378194,\n",
              " 0.004257782595232129,\n",
              " 0.004211214138194919,\n",
              " 0.004165486432611942,\n",
              " 0.004121152218431234,\n",
              " 0.004077323363162577,\n",
              " 0.0040341546991840005,\n",
              " 0.003992079524323344,\n",
              " 0.003950778860598803,\n",
              " 0.003910161438398063,\n",
              " 0.0038702775491401553,\n",
              " 0.0038312134565785527,\n",
              " 0.0037924853386357427,\n",
              " 0.0037547746906057,\n",
              " 0.0037178699858486652,\n",
              " 0.0036814657505601645,\n",
              " 0.003645731834694743,\n",
              " 0.0036103938473388553,\n",
              " 0.0035757245495915413,\n",
              " 0.0035419194027781487,\n",
              " 0.003508701571263373,\n",
              " 0.0034759973641484976,\n",
              " 0.0034436731366440654,\n",
              " 0.0034118511248379946,\n",
              " 0.0033804108388721943,\n",
              " 0.003349783946759999,\n",
              " 0.003319647628813982,\n",
              " 0.003289940534159541,\n",
              " 0.003260458819568157,\n",
              " 0.0032316878205165267,\n",
              " 0.0032033617608249187,\n",
              " 0.0031755100935697556,\n",
              " 0.003147952491417527,\n",
              " 0.0031208645086735487,\n",
              " 0.0030941965524107218,\n",
              " 0.003068002755753696,\n",
              " 0.003042118391022086,\n",
              " 0.003016831004060805,\n",
              " 0.00299172627273947,\n",
              " 0.0029668438946828246,\n",
              " 0.002942460705526173,\n",
              " 0.0029184389859437943,\n",
              " 0.002894839970394969,\n",
              " 0.0028715544613078237,\n",
              " 0.0028485377551987767,\n",
              " 0.002825938514433801,\n",
              " 0.002803531941026449,\n",
              " 0.002781449700705707,\n",
              " 0.002759863738901913,\n",
              " 0.00273841037414968,\n",
              " 0.002717253752052784,\n",
              " 0.00269640376791358,\n",
              " 0.0026758790481835604,\n",
              " 0.002655638149008155,\n",
              " 0.0026355638401582837,\n",
              " 0.002615691046230495,\n",
              " 0.002596164122223854,\n",
              " 0.0025769862113520503,\n",
              " 0.00255804683547467,\n",
              " 0.0025394195690751076,\n",
              " 0.0025208587758243084,\n",
              " 0.00250253698322922,\n",
              " 0.0024845398729667068,\n",
              " 0.0024667782709002495,\n",
              " 0.0024492969969287515,\n",
              " 0.0024317398201674223,\n",
              " 0.00241464760620147,\n",
              " 0.0023977557430043817,\n",
              " 0.0023810474667698145,\n",
              " 0.002364500891417265,\n",
              " 0.002348206238821149,\n",
              " 0.002331995405256748,\n",
              " 0.0023160496493801475,\n",
              " 0.002300356631167233,\n",
              " 0.002284863614477217,\n",
              " 0.0022694687359035015,\n",
              " 0.002254231716506183,\n",
              " 0.0022392256651073694,\n",
              " 0.0022244558203965425,\n",
              " 0.0022098254412412643,\n",
              " 0.0021953280083835125,\n",
              " 0.0021809664322063327,\n",
              " 0.0021667914697900414,\n",
              " 0.002152814995497465,\n",
              " 0.002138942596502602,\n",
              " 0.00212522700894624,\n",
              " 0.002111709676682949,\n",
              " 0.0020983119029551744,\n",
              " 0.0020850717555731535,\n",
              " 0.0020720569882541895,\n",
              " 0.002059177146293223,\n",
              " 0.002046331122983247,\n",
              " 0.0020336912130005658,\n",
              " 0.002021077205426991,\n",
              " 0.0020087258890271187,\n",
              " 0.0019964793464168906,\n",
              " 0.0019843154586851597,\n",
              " 0.00197235937230289,\n",
              " 0.0019604357075877488,\n",
              " 0.0019486896926537156,\n",
              " 0.0019370991503819823,\n",
              " 0.0019255964434705675,\n",
              " 0.0019141390221193433,\n",
              " 0.001902818854432553,\n",
              " 0.001891673426143825,\n",
              " 0.001880654424894601,\n",
              " 0.0018697374034672976,\n",
              " 0.0018588901148177683,\n",
              " 0.0018482355517335236,\n",
              " 0.0018376160296611488,\n",
              " 0.0018271211301907897,\n",
              " 0.001816693227738142,\n",
              " 0.0018064138712361455,\n",
              " 0.001796242839191109,\n",
              " 0.0017861597007140517,\n",
              " 0.0017761122435331345,\n",
              " 0.0017662222380749881,\n",
              " 0.0017564548761583865,\n",
              " 0.0017467321595177054,\n",
              " 0.001737132202833891,\n",
              " 0.0017276547732762992,\n",
              " 0.0017182044102810323,\n",
              " 0.0017088606837205589,\n",
              " 0.001699577842373401,\n",
              " 0.001690444245468825,\n",
              " 0.001681316178292036,\n",
              " 0.0016724264132790267,\n",
              " 0.0016634712228551507,\n",
              " 0.0016546332626603544,\n",
              " 0.001645902288146317,\n",
              " 0.001637233013752848,\n",
              " 0.0016286371392197907,\n",
              " 0.0016201440594159067,\n",
              " 0.001611733459867537,\n",
              " 0.0016033807187341154,\n",
              " 0.0015950602828525007,\n",
              " 0.0015868550399318337,\n",
              " 0.0015786850708536804,\n",
              " 0.0015706137055531144,\n",
              " 0.0015625575324520469,\n",
              " 0.0015546074137091637,\n",
              " 0.001546741696074605,\n",
              " 0.0015389403561130166,\n",
              " 0.0015311965253204107,\n",
              " 0.0015234994934871793,\n",
              " 0.0015159031026996672,\n",
              " 0.001508412475232035,\n",
              " 0.0015009702765382826,\n",
              " 0.0014935387880541384,\n",
              " 0.0014861919917166233,\n",
              " 0.001478905149269849,\n",
              " 0.0014717138838022947,\n",
              " 0.0014645452029071748,\n",
              " 0.0014574607484973967,\n",
              " 0.0014503831043839455,\n",
              " 0.0014434544136747718,\n",
              " 0.001436510938219726,\n",
              " 0.0014296334120444953,\n",
              " 0.001422884757630527,\n",
              " 0.0014161058934405446,\n",
              " 0.0014094790094532073,\n",
              " 0.001402839261572808]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}