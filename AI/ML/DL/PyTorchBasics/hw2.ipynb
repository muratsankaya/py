{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: PyTorch basics\n",
    "\n",
    "In case you are having a lot of trouble getting Torch installed on your local machine, this notebook should contain everything you need to complete the homework. However, for us to autograde your work, you must copy your code from this notebook back into the correct files in the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def save(filename, **kwargs):\n",
    "    \"\"\"\n",
    "    Save a pytorch object to file\n",
    "    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    You shouldn't need to edit this function.\n",
    "\n",
    "    Arguments:\n",
    "        filename: the file in which to save the object\n",
    "\n",
    "    Possible keyword arguments (kwargs):\n",
    "        epoch: the epoch so far if training\n",
    "        model_state_dict: a model's state\n",
    "        opt_state_dict: a optimizer's state, if training\n",
    "    \"\"\"\n",
    "\n",
    "    msg = f\"{filename} exists: delete it first to replace it.\"\n",
    "    assert not os.path.exists(filename), msg\n",
    "    torch.save(kwargs, filename)\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\"\n",
    "    Load a pytorch object from a given filename\n",
    "    See: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    You shouldn't need to edit this function.\n",
    "\n",
    "    Arguments:\n",
    "        filename: the file from which to load the object\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AddDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_examples):\n",
    "        \"\"\"\n",
    "        Create a dataset of the form x_1 + x_2 = y\n",
    "\n",
    "        Save the dataset to class variables.\n",
    "        You should use torch tensors of dtype float32.\n",
    "        \"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        data = np.random.randint(-1000, 1000, size=[num_examples, 2])\n",
    "        label = data.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # TODO Convert to torch tensors and save these as class variables\n",
    "        #      so we can load them with self.__getitem__\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "    def __getitem__(self, item_index):\n",
    "        \"\"\"\n",
    "        Allow us to select items with `dataset[0]`\n",
    "        Use the class variables you created in __init__.\n",
    "\n",
    "        Returns (x, y)\n",
    "            x: the data tensor\n",
    "            y: the label tensor\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MultiplyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_examples):\n",
    "        \"\"\"\n",
    "        Create a dataset of the form x_1 * x_2 = y\n",
    "\n",
    "        Save the dataset to class variables.\n",
    "        You should use torch tensors of dtype float32.\n",
    "        \"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        data = np.random.randint(1, 1000, size=[num_examples, 2])\n",
    "        label = data.prod(axis=1, keepdims=True)\n",
    "\n",
    "        # TODO Convert to torch tensors and save these as class variables\n",
    "        #      so we can load them with self.__getitem__\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "    def __getitem__(self, item_index):\n",
    "        \"\"\"\n",
    "        Allow us to select items with `dataset[0]`\n",
    "        Returns (x, y)\n",
    "            x: the data tensor\n",
    "            y: the label tensor\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/mlp.py\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, number_of_hidden_layers: int, input_size: int,\n",
    "                 hidden_size: int, activation: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Construct a simple MLP\n",
    "        \"\"\"\n",
    "        # NOTE: don't edit this constructor\n",
    "\n",
    "        super().__init__()\n",
    "        assert number_of_hidden_layers >= 0, \"number_of_hidden_layers must be at least 0\"\n",
    "\n",
    "        dims_in = [input_size] + [hidden_size] * number_of_hidden_layers\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [1]\n",
    "\n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "\n",
    "            # No final activation\n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize all the model's weights.\n",
    "        See https://pytorch.org/docs/stable/nn.init.html\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Use `src.utils.save` to save this model to file.\n",
    "\n",
    "        Note: You may want to save a dictionary containing the model's state.\n",
    "\n",
    "        Args\n",
    "            filename: the file to which to save the model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"\n",
    "        Use `src.utils.load` to load this model from file.\n",
    "\n",
    "        Note: in addition to simply loading the saved model, you must use the\n",
    "              information from that checkpoint to update the model's state.\n",
    "\n",
    "        Args\n",
    "            filename: the file from which to load the model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/trainer.py\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, optimizer, model, loss_func, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer for the model, using any necessary kwargs\n",
    "        Save the model and loss function for later calculation\n",
    "        You shouldn't need to edit this function.\n",
    "        \"\"\"\n",
    "\n",
    "        self.optimizer = optimizer(model.parameters(), **kwargs)\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def run_one_batch(self, x, y, train=True):\n",
    "        \"\"\"\n",
    "        Run self.model on one batch of data, using `self.loss_func` to\n",
    "            compute the model's loss.\n",
    "\n",
    "        If train=True (the default), you should use `self.optimizer`\n",
    "            to update the parameters of `self.model`.\n",
    "\n",
    "        You should also call `self.optimizer.zero_grad()`; see\n",
    "            https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "            for a guide as to when to do that.\n",
    "\n",
    "        Args\n",
    "            x: the batch's input\n",
    "            y: the batch's target\n",
    "\n",
    "        Returns\n",
    "            loss: the model's loss on this batch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_one_epoch(self, data_loader: torch.utils.data.DataLoader,\n",
    "                        train=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Train one epoch, a batch at a time, using self.run_one_batch\n",
    "        You shouldn't need to edit this function.\n",
    "\n",
    "        Args:\n",
    "            data_loader: a torch.utils.data.DataLoader with our dataset\n",
    "            stats: an optional dict of information to print out\n",
    "\n",
    "        Returns:\n",
    "            total_loss: the average loss per example\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "        epoch_size = 0\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            x, y = batch_data\n",
    "            epoch_size += x.size(0)\n",
    "            loss = self.run_one_batch(x, y, train=train)\n",
    "            total_loss += loss\n",
    "\n",
    "        avg_loss = total_loss / epoch_size\n",
    "\n",
    "        if verbose:\n",
    "            epoch = self.epoch + 1\n",
    "            duration = (time.time() - self.start_time) / 60\n",
    "\n",
    "            if train:\n",
    "                log = [f\"Epoch: {epoch:6d}\"]\n",
    "            else:\n",
    "                log = [\"Eval:\" + \" \" * 8]\n",
    "\n",
    "            log.extend([\n",
    "                f\"Loss: {avg_loss:6.3f}\",\n",
    "                f\"in {duration:5.1f} min\",\n",
    "            ])\n",
    "            print(\"  \".join(log))\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self, data_loader, n_epochs, train=True, report_every=None):\n",
    "        \"\"\"\n",
    "        Run the model for `n_epochs` epochs on the data in `data_loader`\n",
    "        You shouldn't need to edit this function.\n",
    "\n",
    "        Args\n",
    "            data_loader: data loader for our data\n",
    "            n_epochs: how many epochs to run\n",
    "            train: if True, train the model; otherwise, just evaluate it\n",
    "            report_every: how often to print out stats\n",
    "\n",
    "        Returns\n",
    "            losses: average loss per epoch\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        if report_every is None:\n",
    "            report_every = max(1, n_epochs // 10)\n",
    "\n",
    "        losses = []\n",
    "        for i in range(n_epochs):\n",
    "            verbose = ((i + 1) % report_every) == 0\n",
    "            loss = self.run_one_epoch(data_loader, train=train, verbose=verbose)\n",
    "            losses.append(loss)\n",
    "            if train:\n",
    "                self.epoch += 1\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def eval(self, data_loader):\n",
    "        \"\"\"\n",
    "        Helper function to run through the data loader once and just\n",
    "            compute the loss\n",
    "        You shouldn't need to edit this function.\n",
    "        \"\"\"\n",
    "        return self.train(data_loader, 1, train=False, report_every=1)\n",
    "\n",
    "    def save_trainer(self, filename):\n",
    "        \"\"\"\n",
    "        Use `src.data.save` to save this Trainer to file.\n",
    "        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "        Args\n",
    "            filename: the file to which to save the trainer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_trainer(self, filename):\n",
    "        \"\"\"\n",
    "        Use `src.data.load` to load this trainer from file.\n",
    "        See https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "        Note: in addition to simply loading the saved model, you must\n",
    "            use the information from that checkpoint to update the model's\n",
    "            state.\n",
    "\n",
    "        Args\n",
    "            filename: the file from which to load the model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/experiments.py\n",
    "\n",
    "def params_add_dataset():\n",
    "    \"\"\"\n",
    "    Choose the parameters you used to train your AddDataset model\n",
    "    This will be used to load the model you saved.\n",
    "\n",
    "    Returns\n",
    "        model_args: a dictionary of arguments to be passed to MLP()\n",
    "        trainer_args: a dictionary of arguments to be passed to Trainer()\n",
    "    \"\"\"\n",
    "\n",
    "    model_args = {}\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Don't include 'model' or 'loss_func' here\n",
    "    # Just \"optimizer\" and any necessary kwargs\n",
    "    trainer_args = {}\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return model_args, trainer_args\n",
    "\n",
    "\n",
    "def params_multiply_dataset():\n",
    "    \"\"\"\n",
    "    Choose the parameters you used to train your MultiplyDataset model\n",
    "    This will be used to load the model you saved.\n",
    "\n",
    "    Returns\n",
    "        model_args: a dictionary of arguments to be passed to MLP()\n",
    "        trainer_args: a dictionary of arguments to be passed to Trainer()\n",
    "    \"\"\"\n",
    "\n",
    "    model_args = {}\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Don't include 'model' or 'loss_func' here\n",
    "    # Just \"optimizer\" and any necessary kwargs\n",
    "    trainer_args = {}\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return model_args, trainer_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code\n",
    "\n",
    "You will need to copy `models/test_load_model.pt` to your Colab environment to pass `test_load_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_data.py\n",
    "\n",
    "def test_datasets():\n",
    "    n = 1000\n",
    "    msg = \"Your dataset should provide tensors of type float32\"\n",
    "    for dataset_cls in [AddDataset, MultiplyDataset]:\n",
    "        # Iterate through manually\n",
    "        dataset = dataset_cls(num_examples=n)\n",
    "        count = 0\n",
    "        for i in range(len(dataset)):\n",
    "            x, y = dataset[i]\n",
    "            assert isinstance(x, torch.Tensor), msg\n",
    "            assert x.dtype == torch.float32, msg\n",
    "            assert isinstance(y, torch.Tensor), msg\n",
    "            count += 1\n",
    "        assert count == n\n",
    "\n",
    "        # Iterate through using a DataLoader\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=10, shuffle=False)\n",
    "        count = 0\n",
    "        for batch in data_loader:\n",
    "            x, y = batch\n",
    "            assert isinstance(x, torch.Tensor), msg\n",
    "            assert x.dtype == torch.float32, msg\n",
    "            assert isinstance(y, torch.Tensor), msg\n",
    "            count += 1\n",
    "        assert count == n // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_trainer.py\n",
    "\n",
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Linear(1, 1, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.weights(torch.ones([1, 1], dtype=torch.float32))\n",
    "\n",
    "\n",
    "def test_trainer_basics():\n",
    "    n = 100\n",
    "    target = (torch.zeros(1, dtype=torch.float32),\n",
    "              torch.zeros(1, dtype=torch.float32))\n",
    "\n",
    "    data = [target for _ in range(n)]\n",
    "    model = DummyModel(n)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        optimizer=torch.optim.SGD,\n",
    "        model=model,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        lr=0.1,\n",
    "    )\n",
    "\n",
    "    # Test `run_one_batch` with a dummy example\n",
    "    loss_before = trainer.eval(data_loader)[0]\n",
    "    trainer.run_one_batch(\n",
    "        None, torch.zeros([1, 1], dtype=torch.float32))\n",
    "    loss_after = trainer.eval(data_loader)[0]\n",
    "    assert loss_after < loss_before, \"Loss should decrease\"\n",
    "\n",
    "    # train=False\n",
    "    loss_again = trainer.train(data_loader, 10, train=False, report_every=100)\n",
    "    msg = \"train=False should mean no training\"\n",
    "    assert np.all(np.isclose(loss_after, loss_again)), msg\n",
    "\n",
    "    # Test trainer.train with train=True\n",
    "    _ = trainer.train(data_loader, 3, report_every=100)\n",
    "    losses = trainer.eval(data_loader)\n",
    "    msg = \"DummyModel should learn zero weights\"\n",
    "    assert np.isclose(losses[0], 0)\n",
    "\n",
    "    model_weight = model.weights.weight[0][0].detach()\n",
    "    assert np.isclose(model_weight, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_save_load.py\n",
    "\n",
    "model_args = {\n",
    "    \"number_of_hidden_layers\": 1,\n",
    "    \"input_size\": 2,\n",
    "    \"hidden_size\": 1,\n",
    "    \"activation\": torch.nn.Sigmoid(),\n",
    "}\n",
    "\n",
    "trainer_args = {\n",
    "    \"optimizer\": torch.optim.SGD,\n",
    "    \"loss_func\": torch.nn.MSELoss(),\n",
    "    \"lr\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "def test_load_model():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "\n",
    "    # This model is provided with the repository;\n",
    "    #    you don't need to modify it\n",
    "    fn = f\"models/test_load_model.pt\"\n",
    "    model.load_model(fn)\n",
    "\n",
    "    # It should load correctly\n",
    "    first_layer = model.net[0].weight.detach().numpy()\n",
    "    reference = np.array([0.449, 0.19120623])\n",
    "    assert np.allclose(first_layer[0], reference)\n",
    "\n",
    "\n",
    "def test_save_load_model():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = AddDataset(num_examples=100)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    # Train model for 2 epochs\n",
    "    trainer = Trainer(model=model, **trainer_args)\n",
    "    trainer.train(data_loader, 2)\n",
    "    before = trainer.eval(data_loader)[0]\n",
    "\n",
    "    # Save model\n",
    "    rand = np.random.randint(10000, 99999)\n",
    "    fn = f\"models/model_{rand}.pt\"\n",
    "    model.save_model(fn)\n",
    "\n",
    "    try:\n",
    "        # Load model\n",
    "        model = MLP(**model_args)\n",
    "        model.load_model(fn)\n",
    "\n",
    "        # Loss should be same before/after loading\n",
    "        after = trainer.eval(data_loader)[0]\n",
    "        assert np.isclose(before, after)\n",
    "    finally:\n",
    "        os.remove(fn)\n",
    "\n",
    "\n",
    "def test_continue_training():\n",
    "    # Train for ten epochs, save per-epoch losses\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = AddDataset(num_examples=100)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    trainer = Trainer(model=model, **trainer_args)\n",
    "    ten_losses = trainer.train(data_loader, 10)\n",
    "\n",
    "    # Start over and train for just five epochs\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    dataset = AddDataset(num_examples=100)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    trainer = Trainer(model=model, **trainer_args)\n",
    "    five_losses = trainer.train(data_loader, 5)\n",
    "\n",
    "    try:\n",
    "        # Save the model and trainer\n",
    "        now = re.sub(\"[ :.-]\", \"_\", str(datetime.datetime.now()))\n",
    "        model_fn = f\"models/test_model_{now}.pt\"\n",
    "        trainer_fn = f\"models/test_trainer_{now}.pt\"\n",
    "        model.save_model(model_fn)\n",
    "        trainer.save_trainer(trainer_fn)\n",
    "\n",
    "        # Reload the model and the trainer\n",
    "        model = MLP(**model_args)\n",
    "        model.load_model(model_fn)\n",
    "        trainer = Trainer(model=model, **trainer_args)\n",
    "        trainer.load_trainer(trainer_fn)\n",
    "\n",
    "        # Train for five more epochs\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "        five_more = trainer.train(data_loader, 5)\n",
    "\n",
    "        # Loss should be the same whether you train once for ten\n",
    "        #   epochs or twice for five epochs\n",
    "        assert np.all(np.isclose(ten_losses, five_losses + five_more))\n",
    "    finally:\n",
    "        if os.path.exists(model_fn):\n",
    "            os.remove(model_fn)\n",
    "        if os.path.exists(trainer_fn):\n",
    "            os.remove(trainer_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_model.py\n",
    "\n",
    "def test_model_init():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    model_args = {\n",
    "        \"number_of_hidden_layers\": 1,\n",
    "        \"input_size\": 2,\n",
    "        \"hidden_size\": 10,\n",
    "        \"activation\": torch.nn.Sigmoid(),\n",
    "    }\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.net[0].weight = torch.nn.Parameter(100 * torch.ones(10, 1))\n",
    "    before = model.net[0].weight.detach().numpy().copy()\n",
    "\n",
    "    model.initialize()\n",
    "    after = model.net[0].weight.detach().numpy()\n",
    "\n",
    "    # However you initialize, it should be with small\n",
    "    #   numbers but not all zeros\n",
    "    assert np.std(after) > 0\n",
    "    assert np.abs(np.mean(after)) < 2\n",
    "\n",
    "\n",
    "def test_add_dataset():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = AddDataset(num_examples=1000)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    # Choose your model's parameters in\n",
    "    # src.experiment.py:params_add_dataset()\n",
    "    #   to train a model that can do addition\n",
    "    model_args, trainer_args = params_add_dataset()\n",
    "    msg = \"Must use torch.nn.MSELoss()\"\n",
    "    assert \"loss_func\" not in trainer_args, msg\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        **trainer_args)\n",
    "\n",
    "    _ = trainer.train(data_loader, 200)\n",
    "    losses = trainer.eval(data_loader)\n",
    "    msg = \"Should learn AddDataset in 200 epochs\"\n",
    "    assert losses[0] < 0.1, msg\n",
    "\n",
    "\n",
    "def test_saved_add_dataset():\n",
    "\n",
    "    # Save your model here\n",
    "    MODEL_FN = \"models/test_saved_add_dataset.pt\"\n",
    "    msg = f\"Save your model to {MODEL_FN}\"\n",
    "    assert os.path.exists(MODEL_FN), msg\n",
    "    msg = f\"Delete {MODEL_FN} and then save your model there.\"\n",
    "    assert os.path.getsize(MODEL_FN) > 0, msg\n",
    "\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = AddDataset(num_examples=1000)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    # Choose your model's parameters in\n",
    "    # src.experiment.py:params_add_dataset()\n",
    "    #   to train a model that can do addition\n",
    "    # We need to use the same model architecture\n",
    "    #   to load your saved model\n",
    "    model_args, trainer_args = params_add_dataset()\n",
    "    msg = \"Must use torch.nn.MSELoss()\"\n",
    "    assert \"loss_func\" not in trainer_args, msg\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.load_model(MODEL_FN)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=print,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "    )\n",
    "\n",
    "    losses = trainer.eval(data_loader)\n",
    "    msg = \"Saved model should solve AddDataset\"\n",
    "    assert losses[0] < 0.1, msg\n",
    "\n",
    "\n",
    "def test_saved_multiply_dataset():\n",
    "\n",
    "    # Save your model here\n",
    "    MODEL_FN = \"models/test_saved_multiply_dataset.pt\"\n",
    "    msg = f\"Save your model to {MODEL_FN}\"\n",
    "    assert os.path.exists(MODEL_FN), msg\n",
    "    msg = f\"Delete {MODEL_FN} and then save your model there.\"\n",
    "    assert os.path.getsize(MODEL_FN) > 0, msg\n",
    "\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = MultiplyDataset(num_examples=100)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    # Choose your model's parameters in\n",
    "    # src.experiment.py:params_multiply_dataset()\n",
    "    #   to train a model that can do multiplication\n",
    "    # We need to use the same model architecture\n",
    "    #   to load your saved model\n",
    "    model_args, trainer_args = params_multiply_dataset()\n",
    "    msg = \"Must use torch.nn.MSELoss()\"\n",
    "    assert \"loss_func\" not in trainer_args, msg\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.load_model(MODEL_FN)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=print,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "    )\n",
    "\n",
    "    losses = trainer.eval(data_loader)\n",
    "    msg = \"Saved model is expected to do poorly but not as bad as initially\"\n",
    "    assert losses[0] < 4.1e8, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/save_model_without_deleting.py\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This is a demo function provided just to highlight how you might\n",
    "    train and save your models to pass the `test_saved_add_dataset`\n",
    "    and `test_saved_multiply_dataset` cases.\n",
    "   \n",
    "    You may want to modify the number of examples and number of training\n",
    "    epochs used in `trainer.train(...)`.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    outfn = \"models/test_saved_add_dataset.pt\"\n",
    "    if os.path.exists(outfn):\n",
    "        choice = input(f\"Delete {outfn}? y/n \")\n",
    "        if choice == \"y\":\n",
    "            os.remove(outfn)\n",
    "\n",
    "    dataset = AddDataset(num_examples=100)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model_args, trainer_args = params_add_dataset()\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        **trainer_args)\n",
    "\n",
    "    _ = trainer.train(data_loader, 10)\n",
    "    losses = trainer.eval(data_loader)\n",
    "\n",
    "    model.save_model(outfn)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-response code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free_response/batch_sizes.py\n",
    "\n",
    "def add_dataset_experiment(num_examples=1000, batch_size=100):\n",
    "    \"\"\"\n",
    "    This is a copy of the `test_add_dataset` case in tests/test_model.py,\n",
    "        set up to allow you easily tweak the batch size and dataset size.\n",
    "    Run this script from the root directory of your repository with:\n",
    "        `python -m free_response.batch_sizes <num_examples> <batch_size>`\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    dataset = AddDataset(num_examples=num_examples)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model_args, trainer_args = params_add_dataset()\n",
    "\n",
    "    model = MLP(**model_args)\n",
    "    model.initialize()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        **trainer_args)\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train(data_loader, 100, report_every=100)\n",
    "    end = time.time() - start_time\n",
    "    print(f\"Training took {end:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments you can run:\n",
    "\n",
    "add_dataset_experiment(1000, 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
